{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path = r'movenet.tflite')\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flipping Training Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flipped and saved: 0_960x540.jpg\n",
      "Flipped and saved: 0_960x540.jpg_flipped.jpg\n",
      "Flipped and saved: 0_960x540.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: 2potEB8qNMABrHqjfWIpOOvbyAbsNmbLb7_SAM9jEQE.jpg\n",
      "Flipped and saved: 2potEB8qNMABrHqjfWIpOOvbyAbsNmbLb7_SAM9jEQE.jpg_flipped.jpg\n",
      "Flipped and saved: 2potEB8qNMABrHqjfWIpOOvbyAbsNmbLb7_SAM9jEQE.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: 362-1.jpg\n",
      "Flipped and saved: 362-1.jpg_flipped.jpg\n",
      "Flipped and saved: 362-1.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: 524.jpg\n",
      "Flipped and saved: 524.jpg_flipped.jpg\n",
      "Flipped and saved: 524.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: 6181ae52a3e0153fba71466fc28cb1fc595b4647_720.jpg\n",
      "Flipped and saved: 6181ae52a3e0153fba71466fc28cb1fc595b4647_720.jpg_flipped.jpg\n",
      "Flipped and saved: 6181ae52a3e0153fba71466fc28cb1fc595b4647_720.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: Barbell-Back-Squat.jpg\n",
      "Flipped and saved: Barbell-Back-Squat.jpg_flipped.jpg\n",
      "Flipped and saved: Barbell-Back-Squat.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: Blog-Image-7-STANDING-AB-EXERCISES-THAT-WILL-STRENGTHEN-YOUR-CORE.jpg\n",
      "Flipped and saved: Blog-Image-7-STANDING-AB-EXERCISES-THAT-WILL-STRENGTHEN-YOUR-CORE.jpg_flipped.jpg\n",
      "Flipped and saved: Blog-Image-7-STANDING-AB-EXERCISES-THAT-WILL-STRENGTHEN-YOUR-CORE.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: f2-barbell-back-squat.jpg\n",
      "Flipped and saved: f2-barbell-back-squat.jpg_flipped.jpg\n",
      "Flipped and saved: f2-barbell-back-squat.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: hq720 (1).jpg\n",
      "Flipped and saved: hq720 (1).jpg_flipped.jpg\n",
      "Flipped and saved: hq720 (1).jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: hq720.jpg\n",
      "Flipped and saved: hq720.jpg_flipped.jpg\n",
      "Flipped and saved: hq720.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: man-1203899_1280.jpg\n",
      "Flipped and saved: man-1203899_1280.jpg_flipped.jpg\n",
      "Flipped and saved: man-1203899_1280.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 181414.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 181414.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 181414.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 181458.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 181458.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 181458.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 181607.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 181607.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 181607.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 181644.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 181644.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 181644.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 181720.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 181720.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 181720.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 182206.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 182206.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 182206.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 182443.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 182443.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 182443.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 182809.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 182809.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 182809.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 182942.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 182942.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 182942.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 183554.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 183554.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 183554.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 183631.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 183631.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 183631.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 183709.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 183709.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 183709.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 183735.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 183735.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 183735.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: Squat.StartPosition.GastownPhysioPilates.jpg\n",
      "Flipped and saved: Squat.StartPosition.GastownPhysioPilates.jpg_flipped.jpg\n",
      "Flipped and saved: Squat.StartPosition.GastownPhysioPilates.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: standing-position.jpg\n",
      "Flipped and saved: standing-position.jpg_flipped.jpg\n",
      "Flipped and saved: standing-position.jpg_flipped.jpg_flipped.jpg\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def flip_images(input_dir, output_dir):\n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Loop through all the files in the input directory\n",
    "    for filename in os.listdir(input_dir):\n",
    "        # Construct the full file path\n",
    "        img_path = os.path.join(input_dir, filename)\n",
    "        \n",
    "        # Check if the file is an image\n",
    "        if os.path.isfile(img_path) and filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "            # Open the image\n",
    "            img = Image.open(img_path)\n",
    "            \n",
    "            # Flip the image horizontally\n",
    "            flipped_img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            \n",
    "            # Save the flipped image to the output directory\n",
    "            flipped_img.save(os.path.join(output_dir, filename+'_flipped.jpg'))\n",
    "            print(f\"Flipped and saved: {filename}\")\n",
    "\n",
    "# Example usage:\n",
    "input_directory = r'C:\\Users\\2001l\\OneDrive\\Desktop\\squat\\upPosition'\n",
    "output_directory = r'C:\\Users\\2001l\\OneDrive\\Desktop\\squat\\upPosition'\n",
    "\n",
    "flip_images(input_directory, output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe of landmarks of the training Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def landmarks_from_standing_images(input_dir,X,Y):\n",
    "    for file in os.listdir(input_dir):\n",
    "        img_path = os.path.join(input_dir, file)\n",
    "        \n",
    "        # Check if the file is an image\n",
    "        if os.path.isfile(img_path) and file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "            img = Image.open(img_path)\n",
    "            img = tf.image.resize_with_pad(np.expand_dims(img,axis=0), 192, 192)\n",
    "            input_image = tf.cast(img, dtype=tf.float32)\n",
    "\n",
    "            # Setup input and output\n",
    "            input_details = interpreter.get_input_details()\n",
    "            output_details = interpreter.get_output_details()\n",
    "\n",
    "            #make predictions\n",
    "            interpreter.set_tensor(input_details[0]['index'], np.array(input_image))\n",
    "            interpreter.invoke()\n",
    "            keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
    "            landmarks = keypoints_with_scores\n",
    "\n",
    "            X.append(landmarks)\n",
    "            Y.append(0)\n",
    "    return X,Y\n",
    "\n",
    "def landmarks_from_squating_images(input_dir,X,Y):\n",
    "    for file in os.listdir(input_dir):\n",
    "        img_path = os.path.join(input_dir, file)\n",
    "        \n",
    "        # Check if the file is an image\n",
    "        if os.path.isfile(img_path) and file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "            img = Image.open(img_path)\n",
    "            img = tf.image.resize_with_pad(np.expand_dims(img,axis=0), 192, 192)\n",
    "            input_image = tf.cast(img, dtype=tf.float32)\n",
    "\n",
    "            # Setup input and output\n",
    "            input_details = interpreter.get_input_details()\n",
    "            output_details = interpreter.get_output_details()\n",
    "\n",
    "            #make predictions\n",
    "            interpreter.set_tensor(input_details[0]['index'], np.array(input_image))\n",
    "            interpreter.invoke()\n",
    "            keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
    "            landmarks = keypoints_with_scores\n",
    "\n",
    "            X.append(landmarks)\n",
    "            Y.append(1)\n",
    "    return X,Y\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = landmarks_from_standing_images(r'upPosition',[],[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = landmarks_from_squating_images(r'downPosition',X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [1_x, 2_x, 3_x, 4_x, 5_x, 6_x, 7_x, 8_x, 9_x, 10_x, 11_x, 12_x, 13_x, 14_x, 15_x, 16_x, 17_x, 1_y, 2_y, 3_y, 4_y, 5_y, 6_y, 7_y, 8_y, 9_y, 10_y, 11_y, 12_y, 13_y, 14_y, 15_y, 16_y, 17_y, Y]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Generate column names\n",
    "columns = [f\"{i}_x\" for i in range(1, 18)] + [f\"{i}_y\" for i in range(1, 18)] + ['Y']\n",
    "\n",
    "# Create an empty DataFrame with these columns\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Display the DataFrame to verify\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2001l\\AppData\\Local\\Temp\\ipykernel_56372\\1965758734.py:15: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "y = 0\n",
    "for entry in X:\n",
    "    new_row = {col: None for col in columns}\n",
    "    x = 0\n",
    "    for x in range(1,18):\n",
    "        xName = str(x) + '_x'\n",
    "        yName = str(x) + '_y'\n",
    "        new_row[xName] = entry[0][0][x-1][0]\n",
    "        new_row[yName] = entry[0][0][x-1][1]\n",
    "        x = x+1\n",
    "    new_row['Y'] = Y[y]\n",
    "    y = y+1\n",
    "    df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1_x</th>\n",
       "      <th>2_x</th>\n",
       "      <th>3_x</th>\n",
       "      <th>4_x</th>\n",
       "      <th>5_x</th>\n",
       "      <th>6_x</th>\n",
       "      <th>7_x</th>\n",
       "      <th>8_x</th>\n",
       "      <th>9_x</th>\n",
       "      <th>10_x</th>\n",
       "      <th>...</th>\n",
       "      <th>9_y</th>\n",
       "      <th>10_y</th>\n",
       "      <th>11_y</th>\n",
       "      <th>12_y</th>\n",
       "      <th>13_y</th>\n",
       "      <th>14_y</th>\n",
       "      <th>15_y</th>\n",
       "      <th>16_y</th>\n",
       "      <th>17_y</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.332451</td>\n",
       "      <td>0.327286</td>\n",
       "      <td>0.328188</td>\n",
       "      <td>0.335584</td>\n",
       "      <td>0.337321</td>\n",
       "      <td>0.376591</td>\n",
       "      <td>0.380630</td>\n",
       "      <td>0.395435</td>\n",
       "      <td>0.405161</td>\n",
       "      <td>0.371132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.455058</td>\n",
       "      <td>0.564492</td>\n",
       "      <td>0.452833</td>\n",
       "      <td>0.538660</td>\n",
       "      <td>0.492199</td>\n",
       "      <td>0.552743</td>\n",
       "      <td>0.490987</td>\n",
       "      <td>0.564235</td>\n",
       "      <td>0.489883</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.337427</td>\n",
       "      <td>0.330524</td>\n",
       "      <td>0.334669</td>\n",
       "      <td>0.338237</td>\n",
       "      <td>0.339605</td>\n",
       "      <td>0.375162</td>\n",
       "      <td>0.382447</td>\n",
       "      <td>0.390089</td>\n",
       "      <td>0.408159</td>\n",
       "      <td>0.346712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.430101</td>\n",
       "      <td>0.523434</td>\n",
       "      <td>0.440300</td>\n",
       "      <td>0.505220</td>\n",
       "      <td>0.463085</td>\n",
       "      <td>0.507366</td>\n",
       "      <td>0.448752</td>\n",
       "      <td>0.511667</td>\n",
       "      <td>0.434354</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.248382</td>\n",
       "      <td>0.234584</td>\n",
       "      <td>0.236951</td>\n",
       "      <td>0.245007</td>\n",
       "      <td>0.244894</td>\n",
       "      <td>0.292224</td>\n",
       "      <td>0.277356</td>\n",
       "      <td>0.367411</td>\n",
       "      <td>0.338310</td>\n",
       "      <td>0.382827</td>\n",
       "      <td>...</td>\n",
       "      <td>0.713014</td>\n",
       "      <td>0.429093</td>\n",
       "      <td>0.720514</td>\n",
       "      <td>0.516982</td>\n",
       "      <td>0.610946</td>\n",
       "      <td>0.498965</td>\n",
       "      <td>0.643391</td>\n",
       "      <td>0.495000</td>\n",
       "      <td>0.678543</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.252336</td>\n",
       "      <td>0.240113</td>\n",
       "      <td>0.242199</td>\n",
       "      <td>0.246664</td>\n",
       "      <td>0.248041</td>\n",
       "      <td>0.277887</td>\n",
       "      <td>0.293914</td>\n",
       "      <td>0.341917</td>\n",
       "      <td>0.363804</td>\n",
       "      <td>0.387278</td>\n",
       "      <td>...</td>\n",
       "      <td>0.570867</td>\n",
       "      <td>0.278646</td>\n",
       "      <td>0.568406</td>\n",
       "      <td>0.385382</td>\n",
       "      <td>0.474913</td>\n",
       "      <td>0.346625</td>\n",
       "      <td>0.487613</td>\n",
       "      <td>0.319134</td>\n",
       "      <td>0.507263</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.241581</td>\n",
       "      <td>0.232052</td>\n",
       "      <td>0.231702</td>\n",
       "      <td>0.251969</td>\n",
       "      <td>0.244944</td>\n",
       "      <td>0.315338</td>\n",
       "      <td>0.316320</td>\n",
       "      <td>0.416093</td>\n",
       "      <td>0.418072</td>\n",
       "      <td>0.356919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.445377</td>\n",
       "      <td>0.497518</td>\n",
       "      <td>0.468692</td>\n",
       "      <td>0.557133</td>\n",
       "      <td>0.482137</td>\n",
       "      <td>0.591745</td>\n",
       "      <td>0.482272</td>\n",
       "      <td>0.614701</td>\n",
       "      <td>0.470514</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.269335</td>\n",
       "      <td>0.255440</td>\n",
       "      <td>0.262633</td>\n",
       "      <td>0.248578</td>\n",
       "      <td>0.264382</td>\n",
       "      <td>0.315988</td>\n",
       "      <td>0.331319</td>\n",
       "      <td>0.370942</td>\n",
       "      <td>0.472705</td>\n",
       "      <td>0.351692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.424774</td>\n",
       "      <td>0.252384</td>\n",
       "      <td>0.563788</td>\n",
       "      <td>0.183186</td>\n",
       "      <td>0.268853</td>\n",
       "      <td>0.527715</td>\n",
       "      <td>0.553091</td>\n",
       "      <td>0.381434</td>\n",
       "      <td>0.422276</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.152496</td>\n",
       "      <td>0.131350</td>\n",
       "      <td>0.130717</td>\n",
       "      <td>0.147583</td>\n",
       "      <td>0.143705</td>\n",
       "      <td>0.248949</td>\n",
       "      <td>0.246971</td>\n",
       "      <td>0.266599</td>\n",
       "      <td>0.254043</td>\n",
       "      <td>0.241453</td>\n",
       "      <td>...</td>\n",
       "      <td>0.304535</td>\n",
       "      <td>0.161116</td>\n",
       "      <td>0.155328</td>\n",
       "      <td>0.733582</td>\n",
       "      <td>0.631184</td>\n",
       "      <td>0.520522</td>\n",
       "      <td>0.423227</td>\n",
       "      <td>0.661199</td>\n",
       "      <td>0.525378</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.157098</td>\n",
       "      <td>0.132782</td>\n",
       "      <td>0.134116</td>\n",
       "      <td>0.142868</td>\n",
       "      <td>0.145697</td>\n",
       "      <td>0.240384</td>\n",
       "      <td>0.252563</td>\n",
       "      <td>0.251068</td>\n",
       "      <td>0.265584</td>\n",
       "      <td>0.243522</td>\n",
       "      <td>...</td>\n",
       "      <td>0.660548</td>\n",
       "      <td>0.841031</td>\n",
       "      <td>0.831572</td>\n",
       "      <td>0.380024</td>\n",
       "      <td>0.262156</td>\n",
       "      <td>0.575911</td>\n",
       "      <td>0.483287</td>\n",
       "      <td>0.470470</td>\n",
       "      <td>0.330142</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.291138</td>\n",
       "      <td>0.281100</td>\n",
       "      <td>0.280294</td>\n",
       "      <td>0.301488</td>\n",
       "      <td>0.296908</td>\n",
       "      <td>0.372551</td>\n",
       "      <td>0.364195</td>\n",
       "      <td>0.506963</td>\n",
       "      <td>0.483484</td>\n",
       "      <td>0.417078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.579118</td>\n",
       "      <td>0.587216</td>\n",
       "      <td>0.574139</td>\n",
       "      <td>0.729981</td>\n",
       "      <td>0.651935</td>\n",
       "      <td>0.581281</td>\n",
       "      <td>0.571586</td>\n",
       "      <td>0.655428</td>\n",
       "      <td>0.641182</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.287423</td>\n",
       "      <td>0.276282</td>\n",
       "      <td>0.279109</td>\n",
       "      <td>0.294811</td>\n",
       "      <td>0.299027</td>\n",
       "      <td>0.369762</td>\n",
       "      <td>0.377475</td>\n",
       "      <td>0.491012</td>\n",
       "      <td>0.502300</td>\n",
       "      <td>0.388003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.417008</td>\n",
       "      <td>0.412433</td>\n",
       "      <td>0.400359</td>\n",
       "      <td>0.338844</td>\n",
       "      <td>0.260979</td>\n",
       "      <td>0.428291</td>\n",
       "      <td>0.424024</td>\n",
       "      <td>0.353666</td>\n",
       "      <td>0.351520</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         1_x       2_x       3_x       4_x       5_x       6_x       7_x  \\\n",
       "0   0.332451  0.327286  0.328188  0.335584  0.337321  0.376591  0.380630   \n",
       "1   0.337427  0.330524  0.334669  0.338237  0.339605  0.375162  0.382447   \n",
       "2   0.248382  0.234584  0.236951  0.245007  0.244894  0.292224  0.277356   \n",
       "3   0.252336  0.240113  0.242199  0.246664  0.248041  0.277887  0.293914   \n",
       "4   0.241581  0.232052  0.231702  0.251969  0.244944  0.315338  0.316320   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "91  0.269335  0.255440  0.262633  0.248578  0.264382  0.315988  0.331319   \n",
       "92  0.152496  0.131350  0.130717  0.147583  0.143705  0.248949  0.246971   \n",
       "93  0.157098  0.132782  0.134116  0.142868  0.145697  0.240384  0.252563   \n",
       "94  0.291138  0.281100  0.280294  0.301488  0.296908  0.372551  0.364195   \n",
       "95  0.287423  0.276282  0.279109  0.294811  0.299027  0.369762  0.377475   \n",
       "\n",
       "         8_x       9_x      10_x  ...       9_y      10_y      11_y      12_y  \\\n",
       "0   0.395435  0.405161  0.371132  ...  0.455058  0.564492  0.452833  0.538660   \n",
       "1   0.390089  0.408159  0.346712  ...  0.430101  0.523434  0.440300  0.505220   \n",
       "2   0.367411  0.338310  0.382827  ...  0.713014  0.429093  0.720514  0.516982   \n",
       "3   0.341917  0.363804  0.387278  ...  0.570867  0.278646  0.568406  0.385382   \n",
       "4   0.416093  0.418072  0.356919  ...  0.445377  0.497518  0.468692  0.557133   \n",
       "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
       "91  0.370942  0.472705  0.351692  ...  0.424774  0.252384  0.563788  0.183186   \n",
       "92  0.266599  0.254043  0.241453  ...  0.304535  0.161116  0.155328  0.733582   \n",
       "93  0.251068  0.265584  0.243522  ...  0.660548  0.841031  0.831572  0.380024   \n",
       "94  0.506963  0.483484  0.417078  ...  0.579118  0.587216  0.574139  0.729981   \n",
       "95  0.491012  0.502300  0.388003  ...  0.417008  0.412433  0.400359  0.338844   \n",
       "\n",
       "        13_y      14_y      15_y      16_y      17_y  Y  \n",
       "0   0.492199  0.552743  0.490987  0.564235  0.489883  0  \n",
       "1   0.463085  0.507366  0.448752  0.511667  0.434354  0  \n",
       "2   0.610946  0.498965  0.643391  0.495000  0.678543  0  \n",
       "3   0.474913  0.346625  0.487613  0.319134  0.507263  0  \n",
       "4   0.482137  0.591745  0.482272  0.614701  0.470514  0  \n",
       "..       ...       ...       ...       ...       ... ..  \n",
       "91  0.268853  0.527715  0.553091  0.381434  0.422276  1  \n",
       "92  0.631184  0.520522  0.423227  0.661199  0.525378  1  \n",
       "93  0.262156  0.575911  0.483287  0.470470  0.330142  1  \n",
       "94  0.651935  0.581281  0.571586  0.655428  0.641182  1  \n",
       "95  0.260979  0.428291  0.424024  0.353666  0.351520  1  \n",
       "\n",
       "[96 rows x 35 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'squatTrainSet.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msquatTrainSet.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\2001l\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\2001l\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\generic.py:3964\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3953\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3955\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3956\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3957\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3961\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3962\u001b[0m )\n\u001b[1;32m-> 3964\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3967\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3969\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3972\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3981\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\2001l\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1013\u001b[0m )\n\u001b[1;32m-> 1014\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32mc:\\Users\\2001l\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    268\u001b[0m     )\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32mc:\\Users\\2001l\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'squatTrainSet.csv'"
     ]
    }
   ],
   "source": [
    "df.to_csv('squatTrainSet.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1_x</th>\n",
       "      <th>2_x</th>\n",
       "      <th>3_x</th>\n",
       "      <th>4_x</th>\n",
       "      <th>5_x</th>\n",
       "      <th>6_x</th>\n",
       "      <th>7_x</th>\n",
       "      <th>8_x</th>\n",
       "      <th>9_x</th>\n",
       "      <th>10_x</th>\n",
       "      <th>...</th>\n",
       "      <th>9_y</th>\n",
       "      <th>10_y</th>\n",
       "      <th>11_y</th>\n",
       "      <th>12_y</th>\n",
       "      <th>13_y</th>\n",
       "      <th>14_y</th>\n",
       "      <th>15_y</th>\n",
       "      <th>16_y</th>\n",
       "      <th>17_y</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1_x</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998039</td>\n",
       "      <td>0.997844</td>\n",
       "      <td>0.995700</td>\n",
       "      <td>0.994926</td>\n",
       "      <td>0.952709</td>\n",
       "      <td>0.956751</td>\n",
       "      <td>0.583042</td>\n",
       "      <td>0.588549</td>\n",
       "      <td>0.567609</td>\n",
       "      <td>...</td>\n",
       "      <td>0.224039</td>\n",
       "      <td>-0.104012</td>\n",
       "      <td>0.121761</td>\n",
       "      <td>-0.067104</td>\n",
       "      <td>0.104824</td>\n",
       "      <td>-0.076732</td>\n",
       "      <td>0.138374</td>\n",
       "      <td>-0.164113</td>\n",
       "      <td>0.215662</td>\n",
       "      <td>0.341673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_x</th>\n",
       "      <td>0.998039</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999667</td>\n",
       "      <td>0.997437</td>\n",
       "      <td>0.996860</td>\n",
       "      <td>0.943564</td>\n",
       "      <td>0.947252</td>\n",
       "      <td>0.572502</td>\n",
       "      <td>0.578259</td>\n",
       "      <td>0.575627</td>\n",
       "      <td>...</td>\n",
       "      <td>0.234923</td>\n",
       "      <td>-0.097371</td>\n",
       "      <td>0.129010</td>\n",
       "      <td>-0.079874</td>\n",
       "      <td>0.104086</td>\n",
       "      <td>-0.093967</td>\n",
       "      <td>0.155911</td>\n",
       "      <td>-0.176812</td>\n",
       "      <td>0.219017</td>\n",
       "      <td>0.300589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_x</th>\n",
       "      <td>0.997844</td>\n",
       "      <td>0.999667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996829</td>\n",
       "      <td>0.996853</td>\n",
       "      <td>0.941172</td>\n",
       "      <td>0.946170</td>\n",
       "      <td>0.569052</td>\n",
       "      <td>0.576690</td>\n",
       "      <td>0.576046</td>\n",
       "      <td>...</td>\n",
       "      <td>0.235410</td>\n",
       "      <td>-0.100278</td>\n",
       "      <td>0.129614</td>\n",
       "      <td>-0.084265</td>\n",
       "      <td>0.101632</td>\n",
       "      <td>-0.098475</td>\n",
       "      <td>0.159378</td>\n",
       "      <td>-0.182077</td>\n",
       "      <td>0.220013</td>\n",
       "      <td>0.298172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4_x</th>\n",
       "      <td>0.995700</td>\n",
       "      <td>0.997437</td>\n",
       "      <td>0.996829</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997696</td>\n",
       "      <td>0.954678</td>\n",
       "      <td>0.956737</td>\n",
       "      <td>0.602861</td>\n",
       "      <td>0.604431</td>\n",
       "      <td>0.590388</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228198</td>\n",
       "      <td>-0.105833</td>\n",
       "      <td>0.119764</td>\n",
       "      <td>-0.064791</td>\n",
       "      <td>0.113489</td>\n",
       "      <td>-0.085685</td>\n",
       "      <td>0.141903</td>\n",
       "      <td>-0.166237</td>\n",
       "      <td>0.222908</td>\n",
       "      <td>0.310731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5_x</th>\n",
       "      <td>0.994926</td>\n",
       "      <td>0.996860</td>\n",
       "      <td>0.996853</td>\n",
       "      <td>0.997696</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.951815</td>\n",
       "      <td>0.957361</td>\n",
       "      <td>0.592259</td>\n",
       "      <td>0.601820</td>\n",
       "      <td>0.584815</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232072</td>\n",
       "      <td>-0.088387</td>\n",
       "      <td>0.136466</td>\n",
       "      <td>-0.095793</td>\n",
       "      <td>0.084356</td>\n",
       "      <td>-0.083430</td>\n",
       "      <td>0.145055</td>\n",
       "      <td>-0.188734</td>\n",
       "      <td>0.201952</td>\n",
       "      <td>0.309453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6_x</th>\n",
       "      <td>0.952709</td>\n",
       "      <td>0.943564</td>\n",
       "      <td>0.941172</td>\n",
       "      <td>0.954678</td>\n",
       "      <td>0.951815</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.983856</td>\n",
       "      <td>0.751357</td>\n",
       "      <td>0.742581</td>\n",
       "      <td>0.634628</td>\n",
       "      <td>...</td>\n",
       "      <td>0.173009</td>\n",
       "      <td>-0.064274</td>\n",
       "      <td>0.116608</td>\n",
       "      <td>-0.036414</td>\n",
       "      <td>0.060884</td>\n",
       "      <td>0.060999</td>\n",
       "      <td>0.015900</td>\n",
       "      <td>-0.104399</td>\n",
       "      <td>0.133075</td>\n",
       "      <td>0.466240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7_x</th>\n",
       "      <td>0.956751</td>\n",
       "      <td>0.947252</td>\n",
       "      <td>0.946170</td>\n",
       "      <td>0.956737</td>\n",
       "      <td>0.957361</td>\n",
       "      <td>0.983856</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.732863</td>\n",
       "      <td>0.748849</td>\n",
       "      <td>0.621902</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156591</td>\n",
       "      <td>-0.090410</td>\n",
       "      <td>0.095773</td>\n",
       "      <td>-0.036250</td>\n",
       "      <td>0.064012</td>\n",
       "      <td>0.038773</td>\n",
       "      <td>0.003441</td>\n",
       "      <td>-0.104989</td>\n",
       "      <td>0.141819</td>\n",
       "      <td>0.472527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8_x</th>\n",
       "      <td>0.583042</td>\n",
       "      <td>0.572502</td>\n",
       "      <td>0.569052</td>\n",
       "      <td>0.602861</td>\n",
       "      <td>0.592259</td>\n",
       "      <td>0.751357</td>\n",
       "      <td>0.732863</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.975522</td>\n",
       "      <td>0.735120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031899</td>\n",
       "      <td>-0.037885</td>\n",
       "      <td>0.044048</td>\n",
       "      <td>0.087686</td>\n",
       "      <td>0.048325</td>\n",
       "      <td>0.276466</td>\n",
       "      <td>-0.248980</td>\n",
       "      <td>0.104131</td>\n",
       "      <td>-0.010359</td>\n",
       "      <td>0.397643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9_x</th>\n",
       "      <td>0.588549</td>\n",
       "      <td>0.578259</td>\n",
       "      <td>0.576690</td>\n",
       "      <td>0.604431</td>\n",
       "      <td>0.601820</td>\n",
       "      <td>0.742581</td>\n",
       "      <td>0.748849</td>\n",
       "      <td>0.975522</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.722057</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024481</td>\n",
       "      <td>-0.033608</td>\n",
       "      <td>0.070537</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.026877</td>\n",
       "      <td>0.264549</td>\n",
       "      <td>-0.233267</td>\n",
       "      <td>0.039490</td>\n",
       "      <td>-0.044752</td>\n",
       "      <td>0.403667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10_x</th>\n",
       "      <td>0.567609</td>\n",
       "      <td>0.575627</td>\n",
       "      <td>0.576046</td>\n",
       "      <td>0.590388</td>\n",
       "      <td>0.584815</td>\n",
       "      <td>0.634628</td>\n",
       "      <td>0.621902</td>\n",
       "      <td>0.735120</td>\n",
       "      <td>0.722057</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.147299</td>\n",
       "      <td>0.010622</td>\n",
       "      <td>0.037156</td>\n",
       "      <td>-0.011528</td>\n",
       "      <td>0.027760</td>\n",
       "      <td>0.087406</td>\n",
       "      <td>-0.033764</td>\n",
       "      <td>-0.032226</td>\n",
       "      <td>0.074473</td>\n",
       "      <td>0.105376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11_x</th>\n",
       "      <td>0.565680</td>\n",
       "      <td>0.571988</td>\n",
       "      <td>0.572778</td>\n",
       "      <td>0.585913</td>\n",
       "      <td>0.581263</td>\n",
       "      <td>0.629292</td>\n",
       "      <td>0.626186</td>\n",
       "      <td>0.733013</td>\n",
       "      <td>0.731385</td>\n",
       "      <td>0.988333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104965</td>\n",
       "      <td>0.003374</td>\n",
       "      <td>0.001604</td>\n",
       "      <td>0.008075</td>\n",
       "      <td>0.035826</td>\n",
       "      <td>0.087604</td>\n",
       "      <td>-0.062874</td>\n",
       "      <td>-0.018180</td>\n",
       "      <td>0.066193</td>\n",
       "      <td>0.111526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12_x</th>\n",
       "      <td>0.592896</td>\n",
       "      <td>0.563546</td>\n",
       "      <td>0.560138</td>\n",
       "      <td>0.586273</td>\n",
       "      <td>0.585481</td>\n",
       "      <td>0.739177</td>\n",
       "      <td>0.731525</td>\n",
       "      <td>0.660718</td>\n",
       "      <td>0.668065</td>\n",
       "      <td>0.346804</td>\n",
       "      <td>...</td>\n",
       "      <td>0.116329</td>\n",
       "      <td>-0.123424</td>\n",
       "      <td>0.121921</td>\n",
       "      <td>-0.004365</td>\n",
       "      <td>0.075731</td>\n",
       "      <td>0.165352</td>\n",
       "      <td>-0.110586</td>\n",
       "      <td>-0.051576</td>\n",
       "      <td>0.105997</td>\n",
       "      <td>0.696326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13_x</th>\n",
       "      <td>0.589419</td>\n",
       "      <td>0.559769</td>\n",
       "      <td>0.557102</td>\n",
       "      <td>0.581537</td>\n",
       "      <td>0.584472</td>\n",
       "      <td>0.730986</td>\n",
       "      <td>0.736356</td>\n",
       "      <td>0.655980</td>\n",
       "      <td>0.679000</td>\n",
       "      <td>0.349397</td>\n",
       "      <td>...</td>\n",
       "      <td>0.114082</td>\n",
       "      <td>-0.102836</td>\n",
       "      <td>0.137604</td>\n",
       "      <td>-0.044241</td>\n",
       "      <td>0.032529</td>\n",
       "      <td>0.167728</td>\n",
       "      <td>-0.115771</td>\n",
       "      <td>-0.072006</td>\n",
       "      <td>0.079358</td>\n",
       "      <td>0.697033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14_x</th>\n",
       "      <td>-0.441769</td>\n",
       "      <td>-0.429084</td>\n",
       "      <td>-0.429517</td>\n",
       "      <td>-0.423544</td>\n",
       "      <td>-0.425981</td>\n",
       "      <td>-0.453444</td>\n",
       "      <td>-0.459870</td>\n",
       "      <td>-0.349964</td>\n",
       "      <td>-0.379003</td>\n",
       "      <td>-0.293631</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.207105</td>\n",
       "      <td>-0.019305</td>\n",
       "      <td>-0.181313</td>\n",
       "      <td>0.077215</td>\n",
       "      <td>0.087606</td>\n",
       "      <td>-0.080883</td>\n",
       "      <td>0.010108</td>\n",
       "      <td>0.087392</td>\n",
       "      <td>0.017283</td>\n",
       "      <td>-0.602592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15_x</th>\n",
       "      <td>-0.431585</td>\n",
       "      <td>-0.416593</td>\n",
       "      <td>-0.417294</td>\n",
       "      <td>-0.414643</td>\n",
       "      <td>-0.407054</td>\n",
       "      <td>-0.440863</td>\n",
       "      <td>-0.446945</td>\n",
       "      <td>-0.375400</td>\n",
       "      <td>-0.372154</td>\n",
       "      <td>-0.292173</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.128264</td>\n",
       "      <td>0.128277</td>\n",
       "      <td>-0.020475</td>\n",
       "      <td>-0.111324</td>\n",
       "      <td>-0.094345</td>\n",
       "      <td>-0.100148</td>\n",
       "      <td>0.031371</td>\n",
       "      <td>-0.068749</td>\n",
       "      <td>-0.106134</td>\n",
       "      <td>-0.591942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16_x</th>\n",
       "      <td>-0.632029</td>\n",
       "      <td>-0.647903</td>\n",
       "      <td>-0.649369</td>\n",
       "      <td>-0.636863</td>\n",
       "      <td>-0.646078</td>\n",
       "      <td>-0.555656</td>\n",
       "      <td>-0.564256</td>\n",
       "      <td>-0.311042</td>\n",
       "      <td>-0.341626</td>\n",
       "      <td>-0.463958</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.291666</td>\n",
       "      <td>-0.063608</td>\n",
       "      <td>-0.225808</td>\n",
       "      <td>0.212051</td>\n",
       "      <td>0.094605</td>\n",
       "      <td>0.131388</td>\n",
       "      <td>-0.197203</td>\n",
       "      <td>0.270099</td>\n",
       "      <td>-0.052354</td>\n",
       "      <td>-0.189415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17_x</th>\n",
       "      <td>-0.636251</td>\n",
       "      <td>-0.647424</td>\n",
       "      <td>-0.648953</td>\n",
       "      <td>-0.646465</td>\n",
       "      <td>-0.633328</td>\n",
       "      <td>-0.553761</td>\n",
       "      <td>-0.560282</td>\n",
       "      <td>-0.342377</td>\n",
       "      <td>-0.318922</td>\n",
       "      <td>-0.461896</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.182610</td>\n",
       "      <td>0.203085</td>\n",
       "      <td>0.048680</td>\n",
       "      <td>-0.116758</td>\n",
       "      <td>-0.235379</td>\n",
       "      <td>0.153117</td>\n",
       "      <td>-0.189531</td>\n",
       "      <td>0.014515</td>\n",
       "      <td>-0.318426</td>\n",
       "      <td>-0.182338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_y</th>\n",
       "      <td>0.004096</td>\n",
       "      <td>0.009630</td>\n",
       "      <td>0.006045</td>\n",
       "      <td>-0.001397</td>\n",
       "      <td>0.018088</td>\n",
       "      <td>0.048395</td>\n",
       "      <td>-0.024182</td>\n",
       "      <td>-0.005140</td>\n",
       "      <td>-0.002394</td>\n",
       "      <td>0.031391</td>\n",
       "      <td>...</td>\n",
       "      <td>0.308097</td>\n",
       "      <td>0.377565</td>\n",
       "      <td>0.370423</td>\n",
       "      <td>-0.194667</td>\n",
       "      <td>-0.194687</td>\n",
       "      <td>0.399441</td>\n",
       "      <td>0.356519</td>\n",
       "      <td>-0.016000</td>\n",
       "      <td>-0.047790</td>\n",
       "      <td>-0.001403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_y</th>\n",
       "      <td>-0.038061</td>\n",
       "      <td>-0.035360</td>\n",
       "      <td>-0.039465</td>\n",
       "      <td>-0.042510</td>\n",
       "      <td>-0.027787</td>\n",
       "      <td>0.019871</td>\n",
       "      <td>-0.051154</td>\n",
       "      <td>0.006041</td>\n",
       "      <td>-0.000553</td>\n",
       "      <td>0.018775</td>\n",
       "      <td>...</td>\n",
       "      <td>0.207074</td>\n",
       "      <td>0.350241</td>\n",
       "      <td>0.263931</td>\n",
       "      <td>-0.087185</td>\n",
       "      <td>-0.141095</td>\n",
       "      <td>0.504292</td>\n",
       "      <td>0.285136</td>\n",
       "      <td>0.123655</td>\n",
       "      <td>-0.048275</td>\n",
       "      <td>0.001488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_y</th>\n",
       "      <td>0.041950</td>\n",
       "      <td>0.047612</td>\n",
       "      <td>0.044681</td>\n",
       "      <td>0.036540</td>\n",
       "      <td>0.051565</td>\n",
       "      <td>0.065654</td>\n",
       "      <td>-0.002977</td>\n",
       "      <td>-0.017273</td>\n",
       "      <td>-0.016235</td>\n",
       "      <td>0.029564</td>\n",
       "      <td>...</td>\n",
       "      <td>0.369515</td>\n",
       "      <td>0.263303</td>\n",
       "      <td>0.347406</td>\n",
       "      <td>-0.147198</td>\n",
       "      <td>-0.080523</td>\n",
       "      <td>0.325745</td>\n",
       "      <td>0.471984</td>\n",
       "      <td>-0.009886</td>\n",
       "      <td>0.105435</td>\n",
       "      <td>0.001987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4_y</th>\n",
       "      <td>-0.083785</td>\n",
       "      <td>-0.088091</td>\n",
       "      <td>-0.092618</td>\n",
       "      <td>-0.082418</td>\n",
       "      <td>-0.091614</td>\n",
       "      <td>-0.021421</td>\n",
       "      <td>-0.074711</td>\n",
       "      <td>0.057678</td>\n",
       "      <td>0.006169</td>\n",
       "      <td>0.022306</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081044</td>\n",
       "      <td>0.052027</td>\n",
       "      <td>-0.170472</td>\n",
       "      <td>0.413520</td>\n",
       "      <td>0.269228</td>\n",
       "      <td>0.620484</td>\n",
       "      <td>0.115376</td>\n",
       "      <td>0.587099</td>\n",
       "      <td>0.179473</td>\n",
       "      <td>-0.008896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5_y</th>\n",
       "      <td>0.097904</td>\n",
       "      <td>0.099563</td>\n",
       "      <td>0.098159</td>\n",
       "      <td>0.096881</td>\n",
       "      <td>0.087781</td>\n",
       "      <td>0.081160</td>\n",
       "      <td>0.034719</td>\n",
       "      <td>-0.002960</td>\n",
       "      <td>-0.034674</td>\n",
       "      <td>0.032634</td>\n",
       "      <td>...</td>\n",
       "      <td>0.342031</td>\n",
       "      <td>-0.186473</td>\n",
       "      <td>0.052771</td>\n",
       "      <td>0.265539</td>\n",
       "      <td>0.440107</td>\n",
       "      <td>0.154313</td>\n",
       "      <td>0.587545</td>\n",
       "      <td>0.239917</td>\n",
       "      <td>0.582226</td>\n",
       "      <td>0.018799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6_y</th>\n",
       "      <td>-0.208310</td>\n",
       "      <td>-0.219507</td>\n",
       "      <td>-0.226194</td>\n",
       "      <td>-0.202636</td>\n",
       "      <td>-0.222259</td>\n",
       "      <td>-0.100392</td>\n",
       "      <td>-0.142274</td>\n",
       "      <td>0.112494</td>\n",
       "      <td>0.030076</td>\n",
       "      <td>-0.009669</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.322063</td>\n",
       "      <td>0.095570</td>\n",
       "      <td>-0.384426</td>\n",
       "      <td>0.669838</td>\n",
       "      <td>0.335984</td>\n",
       "      <td>0.709643</td>\n",
       "      <td>-0.261953</td>\n",
       "      <td>0.830128</td>\n",
       "      <td>0.039992</td>\n",
       "      <td>0.006609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7_y</th>\n",
       "      <td>0.239486</td>\n",
       "      <td>0.245339</td>\n",
       "      <td>0.244947</td>\n",
       "      <td>0.239620</td>\n",
       "      <td>0.222154</td>\n",
       "      <td>0.156880</td>\n",
       "      <td>0.129136</td>\n",
       "      <td>-0.024359</td>\n",
       "      <td>-0.067782</td>\n",
       "      <td>0.054557</td>\n",
       "      <td>...</td>\n",
       "      <td>0.541702</td>\n",
       "      <td>-0.371119</td>\n",
       "      <td>0.112466</td>\n",
       "      <td>0.324312</td>\n",
       "      <td>0.673797</td>\n",
       "      <td>-0.233046</td>\n",
       "      <td>0.714950</td>\n",
       "      <td>0.088145</td>\n",
       "      <td>0.848158</td>\n",
       "      <td>0.010294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8_y</th>\n",
       "      <td>-0.197440</td>\n",
       "      <td>-0.199029</td>\n",
       "      <td>-0.203705</td>\n",
       "      <td>-0.198965</td>\n",
       "      <td>-0.193001</td>\n",
       "      <td>-0.117342</td>\n",
       "      <td>-0.142914</td>\n",
       "      <td>0.022007</td>\n",
       "      <td>-0.007473</td>\n",
       "      <td>-0.092103</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042015</td>\n",
       "      <td>0.810877</td>\n",
       "      <td>0.239810</td>\n",
       "      <td>-0.028256</td>\n",
       "      <td>-0.365860</td>\n",
       "      <td>0.669449</td>\n",
       "      <td>-0.332775</td>\n",
       "      <td>0.311317</td>\n",
       "      <td>-0.520879</td>\n",
       "      <td>-0.048657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9_y</th>\n",
       "      <td>0.224039</td>\n",
       "      <td>0.234923</td>\n",
       "      <td>0.235410</td>\n",
       "      <td>0.228198</td>\n",
       "      <td>0.232072</td>\n",
       "      <td>0.173009</td>\n",
       "      <td>0.156591</td>\n",
       "      <td>0.031899</td>\n",
       "      <td>0.024481</td>\n",
       "      <td>0.147299</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.230420</td>\n",
       "      <td>0.808456</td>\n",
       "      <td>-0.339347</td>\n",
       "      <td>-0.004345</td>\n",
       "      <td>-0.310763</td>\n",
       "      <td>0.677365</td>\n",
       "      <td>-0.482842</td>\n",
       "      <td>0.361664</td>\n",
       "      <td>0.070515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10_y</th>\n",
       "      <td>-0.104012</td>\n",
       "      <td>-0.097371</td>\n",
       "      <td>-0.100278</td>\n",
       "      <td>-0.105833</td>\n",
       "      <td>-0.088387</td>\n",
       "      <td>-0.064274</td>\n",
       "      <td>-0.090410</td>\n",
       "      <td>-0.037885</td>\n",
       "      <td>-0.033608</td>\n",
       "      <td>0.010622</td>\n",
       "      <td>...</td>\n",
       "      <td>0.230420</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.557468</td>\n",
       "      <td>-0.414418</td>\n",
       "      <td>-0.615452</td>\n",
       "      <td>0.360414</td>\n",
       "      <td>-0.109192</td>\n",
       "      <td>-0.151245</td>\n",
       "      <td>-0.564731</td>\n",
       "      <td>-0.074850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11_y</th>\n",
       "      <td>0.121761</td>\n",
       "      <td>0.129010</td>\n",
       "      <td>0.129614</td>\n",
       "      <td>0.119764</td>\n",
       "      <td>0.136466</td>\n",
       "      <td>0.116608</td>\n",
       "      <td>0.095773</td>\n",
       "      <td>0.044048</td>\n",
       "      <td>0.070537</td>\n",
       "      <td>0.037156</td>\n",
       "      <td>...</td>\n",
       "      <td>0.808456</td>\n",
       "      <td>0.557468</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.620337</td>\n",
       "      <td>-0.428816</td>\n",
       "      <td>-0.110760</td>\n",
       "      <td>0.385957</td>\n",
       "      <td>-0.584657</td>\n",
       "      <td>-0.110985</td>\n",
       "      <td>0.094309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12_y</th>\n",
       "      <td>-0.067104</td>\n",
       "      <td>-0.079874</td>\n",
       "      <td>-0.084265</td>\n",
       "      <td>-0.064791</td>\n",
       "      <td>-0.095793</td>\n",
       "      <td>-0.036414</td>\n",
       "      <td>-0.036250</td>\n",
       "      <td>0.087686</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.011528</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.339347</td>\n",
       "      <td>-0.414418</td>\n",
       "      <td>-0.620337</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.838302</td>\n",
       "      <td>0.176672</td>\n",
       "      <td>-0.217986</td>\n",
       "      <td>0.813845</td>\n",
       "      <td>0.474736</td>\n",
       "      <td>0.053299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13_y</th>\n",
       "      <td>0.104824</td>\n",
       "      <td>0.104086</td>\n",
       "      <td>0.101632</td>\n",
       "      <td>0.113489</td>\n",
       "      <td>0.084356</td>\n",
       "      <td>0.060884</td>\n",
       "      <td>0.064012</td>\n",
       "      <td>0.048325</td>\n",
       "      <td>-0.026877</td>\n",
       "      <td>0.027760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004345</td>\n",
       "      <td>-0.615452</td>\n",
       "      <td>-0.428816</td>\n",
       "      <td>0.838302</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.200480</td>\n",
       "      <td>0.181325</td>\n",
       "      <td>0.504195</td>\n",
       "      <td>0.812456</td>\n",
       "      <td>-0.027717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14_y</th>\n",
       "      <td>-0.076732</td>\n",
       "      <td>-0.093967</td>\n",
       "      <td>-0.098475</td>\n",
       "      <td>-0.085685</td>\n",
       "      <td>-0.083430</td>\n",
       "      <td>0.060999</td>\n",
       "      <td>0.038773</td>\n",
       "      <td>0.276466</td>\n",
       "      <td>0.264549</td>\n",
       "      <td>0.087406</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.310763</td>\n",
       "      <td>0.360414</td>\n",
       "      <td>-0.110760</td>\n",
       "      <td>0.176672</td>\n",
       "      <td>-0.200480</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.458620</td>\n",
       "      <td>0.616636</td>\n",
       "      <td>-0.409062</td>\n",
       "      <td>0.204168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15_y</th>\n",
       "      <td>0.138374</td>\n",
       "      <td>0.155911</td>\n",
       "      <td>0.159378</td>\n",
       "      <td>0.141903</td>\n",
       "      <td>0.145055</td>\n",
       "      <td>0.015900</td>\n",
       "      <td>0.003441</td>\n",
       "      <td>-0.248980</td>\n",
       "      <td>-0.233267</td>\n",
       "      <td>-0.033764</td>\n",
       "      <td>...</td>\n",
       "      <td>0.677365</td>\n",
       "      <td>-0.109192</td>\n",
       "      <td>0.385957</td>\n",
       "      <td>-0.217986</td>\n",
       "      <td>0.181325</td>\n",
       "      <td>-0.458620</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.414223</td>\n",
       "      <td>0.643928</td>\n",
       "      <td>-0.147063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16_y</th>\n",
       "      <td>-0.164113</td>\n",
       "      <td>-0.176812</td>\n",
       "      <td>-0.182077</td>\n",
       "      <td>-0.166237</td>\n",
       "      <td>-0.188734</td>\n",
       "      <td>-0.104399</td>\n",
       "      <td>-0.104989</td>\n",
       "      <td>0.104131</td>\n",
       "      <td>0.039490</td>\n",
       "      <td>-0.032226</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.482842</td>\n",
       "      <td>-0.151245</td>\n",
       "      <td>-0.584657</td>\n",
       "      <td>0.813845</td>\n",
       "      <td>0.504195</td>\n",
       "      <td>0.616636</td>\n",
       "      <td>-0.414223</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.102966</td>\n",
       "      <td>0.028712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17_y</th>\n",
       "      <td>0.215662</td>\n",
       "      <td>0.219017</td>\n",
       "      <td>0.220013</td>\n",
       "      <td>0.222908</td>\n",
       "      <td>0.201952</td>\n",
       "      <td>0.133075</td>\n",
       "      <td>0.141819</td>\n",
       "      <td>-0.010359</td>\n",
       "      <td>-0.044752</td>\n",
       "      <td>0.074473</td>\n",
       "      <td>...</td>\n",
       "      <td>0.361664</td>\n",
       "      <td>-0.564731</td>\n",
       "      <td>-0.110985</td>\n",
       "      <td>0.474736</td>\n",
       "      <td>0.812456</td>\n",
       "      <td>-0.409062</td>\n",
       "      <td>0.643928</td>\n",
       "      <td>0.102966</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.002311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y</th>\n",
       "      <td>0.341673</td>\n",
       "      <td>0.300589</td>\n",
       "      <td>0.298172</td>\n",
       "      <td>0.310731</td>\n",
       "      <td>0.309453</td>\n",
       "      <td>0.466240</td>\n",
       "      <td>0.472527</td>\n",
       "      <td>0.397643</td>\n",
       "      <td>0.403667</td>\n",
       "      <td>0.105376</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070515</td>\n",
       "      <td>-0.074850</td>\n",
       "      <td>0.094309</td>\n",
       "      <td>0.053299</td>\n",
       "      <td>-0.027717</td>\n",
       "      <td>0.204168</td>\n",
       "      <td>-0.147063</td>\n",
       "      <td>0.028712</td>\n",
       "      <td>-0.002311</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           1_x       2_x       3_x       4_x       5_x       6_x       7_x  \\\n",
       "1_x   1.000000  0.998039  0.997844  0.995700  0.994926  0.952709  0.956751   \n",
       "2_x   0.998039  1.000000  0.999667  0.997437  0.996860  0.943564  0.947252   \n",
       "3_x   0.997844  0.999667  1.000000  0.996829  0.996853  0.941172  0.946170   \n",
       "4_x   0.995700  0.997437  0.996829  1.000000  0.997696  0.954678  0.956737   \n",
       "5_x   0.994926  0.996860  0.996853  0.997696  1.000000  0.951815  0.957361   \n",
       "6_x   0.952709  0.943564  0.941172  0.954678  0.951815  1.000000  0.983856   \n",
       "7_x   0.956751  0.947252  0.946170  0.956737  0.957361  0.983856  1.000000   \n",
       "8_x   0.583042  0.572502  0.569052  0.602861  0.592259  0.751357  0.732863   \n",
       "9_x   0.588549  0.578259  0.576690  0.604431  0.601820  0.742581  0.748849   \n",
       "10_x  0.567609  0.575627  0.576046  0.590388  0.584815  0.634628  0.621902   \n",
       "11_x  0.565680  0.571988  0.572778  0.585913  0.581263  0.629292  0.626186   \n",
       "12_x  0.592896  0.563546  0.560138  0.586273  0.585481  0.739177  0.731525   \n",
       "13_x  0.589419  0.559769  0.557102  0.581537  0.584472  0.730986  0.736356   \n",
       "14_x -0.441769 -0.429084 -0.429517 -0.423544 -0.425981 -0.453444 -0.459870   \n",
       "15_x -0.431585 -0.416593 -0.417294 -0.414643 -0.407054 -0.440863 -0.446945   \n",
       "16_x -0.632029 -0.647903 -0.649369 -0.636863 -0.646078 -0.555656 -0.564256   \n",
       "17_x -0.636251 -0.647424 -0.648953 -0.646465 -0.633328 -0.553761 -0.560282   \n",
       "1_y   0.004096  0.009630  0.006045 -0.001397  0.018088  0.048395 -0.024182   \n",
       "2_y  -0.038061 -0.035360 -0.039465 -0.042510 -0.027787  0.019871 -0.051154   \n",
       "3_y   0.041950  0.047612  0.044681  0.036540  0.051565  0.065654 -0.002977   \n",
       "4_y  -0.083785 -0.088091 -0.092618 -0.082418 -0.091614 -0.021421 -0.074711   \n",
       "5_y   0.097904  0.099563  0.098159  0.096881  0.087781  0.081160  0.034719   \n",
       "6_y  -0.208310 -0.219507 -0.226194 -0.202636 -0.222259 -0.100392 -0.142274   \n",
       "7_y   0.239486  0.245339  0.244947  0.239620  0.222154  0.156880  0.129136   \n",
       "8_y  -0.197440 -0.199029 -0.203705 -0.198965 -0.193001 -0.117342 -0.142914   \n",
       "9_y   0.224039  0.234923  0.235410  0.228198  0.232072  0.173009  0.156591   \n",
       "10_y -0.104012 -0.097371 -0.100278 -0.105833 -0.088387 -0.064274 -0.090410   \n",
       "11_y  0.121761  0.129010  0.129614  0.119764  0.136466  0.116608  0.095773   \n",
       "12_y -0.067104 -0.079874 -0.084265 -0.064791 -0.095793 -0.036414 -0.036250   \n",
       "13_y  0.104824  0.104086  0.101632  0.113489  0.084356  0.060884  0.064012   \n",
       "14_y -0.076732 -0.093967 -0.098475 -0.085685 -0.083430  0.060999  0.038773   \n",
       "15_y  0.138374  0.155911  0.159378  0.141903  0.145055  0.015900  0.003441   \n",
       "16_y -0.164113 -0.176812 -0.182077 -0.166237 -0.188734 -0.104399 -0.104989   \n",
       "17_y  0.215662  0.219017  0.220013  0.222908  0.201952  0.133075  0.141819   \n",
       "Y     0.341673  0.300589  0.298172  0.310731  0.309453  0.466240  0.472527   \n",
       "\n",
       "           8_x       9_x      10_x  ...       9_y      10_y      11_y  \\\n",
       "1_x   0.583042  0.588549  0.567609  ...  0.224039 -0.104012  0.121761   \n",
       "2_x   0.572502  0.578259  0.575627  ...  0.234923 -0.097371  0.129010   \n",
       "3_x   0.569052  0.576690  0.576046  ...  0.235410 -0.100278  0.129614   \n",
       "4_x   0.602861  0.604431  0.590388  ...  0.228198 -0.105833  0.119764   \n",
       "5_x   0.592259  0.601820  0.584815  ...  0.232072 -0.088387  0.136466   \n",
       "6_x   0.751357  0.742581  0.634628  ...  0.173009 -0.064274  0.116608   \n",
       "7_x   0.732863  0.748849  0.621902  ...  0.156591 -0.090410  0.095773   \n",
       "8_x   1.000000  0.975522  0.735120  ...  0.031899 -0.037885  0.044048   \n",
       "9_x   0.975522  1.000000  0.722057  ...  0.024481 -0.033608  0.070537   \n",
       "10_x  0.735120  0.722057  1.000000  ...  0.147299  0.010622  0.037156   \n",
       "11_x  0.733013  0.731385  0.988333  ...  0.104965  0.003374  0.001604   \n",
       "12_x  0.660718  0.668065  0.346804  ...  0.116329 -0.123424  0.121921   \n",
       "13_x  0.655980  0.679000  0.349397  ...  0.114082 -0.102836  0.137604   \n",
       "14_x -0.349964 -0.379003 -0.293631  ... -0.207105 -0.019305 -0.181313   \n",
       "15_x -0.375400 -0.372154 -0.292173  ... -0.128264  0.128277 -0.020475   \n",
       "16_x -0.311042 -0.341626 -0.463958  ... -0.291666 -0.063608 -0.225808   \n",
       "17_x -0.342377 -0.318922 -0.461896  ... -0.182610  0.203085  0.048680   \n",
       "1_y  -0.005140 -0.002394  0.031391  ...  0.308097  0.377565  0.370423   \n",
       "2_y   0.006041 -0.000553  0.018775  ...  0.207074  0.350241  0.263931   \n",
       "3_y  -0.017273 -0.016235  0.029564  ...  0.369515  0.263303  0.347406   \n",
       "4_y   0.057678  0.006169  0.022306  ... -0.081044  0.052027 -0.170472   \n",
       "5_y  -0.002960 -0.034674  0.032634  ...  0.342031 -0.186473  0.052771   \n",
       "6_y   0.112494  0.030076 -0.009669  ... -0.322063  0.095570 -0.384426   \n",
       "7_y  -0.024359 -0.067782  0.054557  ...  0.541702 -0.371119  0.112466   \n",
       "8_y   0.022007 -0.007473 -0.092103  ... -0.042015  0.810877  0.239810   \n",
       "9_y   0.031899  0.024481  0.147299  ...  1.000000  0.230420  0.808456   \n",
       "10_y -0.037885 -0.033608  0.010622  ...  0.230420  1.000000  0.557468   \n",
       "11_y  0.044048  0.070537  0.037156  ...  0.808456  0.557468  1.000000   \n",
       "12_y  0.087686 -0.002592 -0.011528  ... -0.339347 -0.414418 -0.620337   \n",
       "13_y  0.048325 -0.026877  0.027760  ... -0.004345 -0.615452 -0.428816   \n",
       "14_y  0.276466  0.264549  0.087406  ... -0.310763  0.360414 -0.110760   \n",
       "15_y -0.248980 -0.233267 -0.033764  ...  0.677365 -0.109192  0.385957   \n",
       "16_y  0.104131  0.039490 -0.032226  ... -0.482842 -0.151245 -0.584657   \n",
       "17_y -0.010359 -0.044752  0.074473  ...  0.361664 -0.564731 -0.110985   \n",
       "Y     0.397643  0.403667  0.105376  ...  0.070515 -0.074850  0.094309   \n",
       "\n",
       "          12_y      13_y      14_y      15_y      16_y      17_y         Y  \n",
       "1_x  -0.067104  0.104824 -0.076732  0.138374 -0.164113  0.215662  0.341673  \n",
       "2_x  -0.079874  0.104086 -0.093967  0.155911 -0.176812  0.219017  0.300589  \n",
       "3_x  -0.084265  0.101632 -0.098475  0.159378 -0.182077  0.220013  0.298172  \n",
       "4_x  -0.064791  0.113489 -0.085685  0.141903 -0.166237  0.222908  0.310731  \n",
       "5_x  -0.095793  0.084356 -0.083430  0.145055 -0.188734  0.201952  0.309453  \n",
       "6_x  -0.036414  0.060884  0.060999  0.015900 -0.104399  0.133075  0.466240  \n",
       "7_x  -0.036250  0.064012  0.038773  0.003441 -0.104989  0.141819  0.472527  \n",
       "8_x   0.087686  0.048325  0.276466 -0.248980  0.104131 -0.010359  0.397643  \n",
       "9_x  -0.002592 -0.026877  0.264549 -0.233267  0.039490 -0.044752  0.403667  \n",
       "10_x -0.011528  0.027760  0.087406 -0.033764 -0.032226  0.074473  0.105376  \n",
       "11_x  0.008075  0.035826  0.087604 -0.062874 -0.018180  0.066193  0.111526  \n",
       "12_x -0.004365  0.075731  0.165352 -0.110586 -0.051576  0.105997  0.696326  \n",
       "13_x -0.044241  0.032529  0.167728 -0.115771 -0.072006  0.079358  0.697033  \n",
       "14_x  0.077215  0.087606 -0.080883  0.010108  0.087392  0.017283 -0.602592  \n",
       "15_x -0.111324 -0.094345 -0.100148  0.031371 -0.068749 -0.106134 -0.591942  \n",
       "16_x  0.212051  0.094605  0.131388 -0.197203  0.270099 -0.052354 -0.189415  \n",
       "17_x -0.116758 -0.235379  0.153117 -0.189531  0.014515 -0.318426 -0.182338  \n",
       "1_y  -0.194667 -0.194687  0.399441  0.356519 -0.016000 -0.047790 -0.001403  \n",
       "2_y  -0.087185 -0.141095  0.504292  0.285136  0.123655 -0.048275  0.001488  \n",
       "3_y  -0.147198 -0.080523  0.325745  0.471984 -0.009886  0.105435  0.001987  \n",
       "4_y   0.413520  0.269228  0.620484  0.115376  0.587099  0.179473 -0.008896  \n",
       "5_y   0.265539  0.440107  0.154313  0.587545  0.239917  0.582226  0.018799  \n",
       "6_y   0.669838  0.335984  0.709643 -0.261953  0.830128  0.039992  0.006609  \n",
       "7_y   0.324312  0.673797 -0.233046  0.714950  0.088145  0.848158  0.010294  \n",
       "8_y  -0.028256 -0.365860  0.669449 -0.332775  0.311317 -0.520879 -0.048657  \n",
       "9_y  -0.339347 -0.004345 -0.310763  0.677365 -0.482842  0.361664  0.070515  \n",
       "10_y -0.414418 -0.615452  0.360414 -0.109192 -0.151245 -0.564731 -0.074850  \n",
       "11_y -0.620337 -0.428816 -0.110760  0.385957 -0.584657 -0.110985  0.094309  \n",
       "12_y  1.000000  0.838302  0.176672 -0.217986  0.813845  0.474736  0.053299  \n",
       "13_y  0.838302  1.000000 -0.200480  0.181325  0.504195  0.812456 -0.027717  \n",
       "14_y  0.176672 -0.200480  1.000000 -0.458620  0.616636 -0.409062  0.204168  \n",
       "15_y -0.217986  0.181325 -0.458620  1.000000 -0.414223  0.643928 -0.147063  \n",
       "16_y  0.813845  0.504195  0.616636 -0.414223  1.000000  0.102966  0.028712  \n",
       "17_y  0.474736  0.812456 -0.409062  0.643928  0.102966  1.000000 -0.002311  \n",
       "Y     0.053299 -0.027717  0.204168 -0.147063  0.028712 -0.002311  1.000000  \n",
       "\n",
       "[35 rows x 35 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols = [col for col in columns if col != \"Y\"]\n",
    "x_df = df[x_cols]\n",
    "y_df = df[['Y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2001l\\AppData\\Local\\Temp\\ipykernel_56372\\2561977078.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y_df['Y'] = y_df['Y'].astype('int')\n"
     ]
    }
   ],
   "source": [
    "y_df['Y'] = y_df['Y'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Y    int32\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\2001l\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 133ms/step - accuracy: 0.6859 - loss: 0.6479 - val_accuracy: 0.0000e+00 - val_loss: 0.8980\n",
      "Epoch 2/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6507 - loss: 0.6494 - val_accuracy: 0.0000e+00 - val_loss: 0.9315\n",
      "Epoch 3/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6624 - loss: 0.6409 - val_accuracy: 0.0000e+00 - val_loss: 0.9704\n",
      "Epoch 4/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6937 - loss: 0.6209 - val_accuracy: 0.0000e+00 - val_loss: 0.9938\n",
      "Epoch 5/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6937 - loss: 0.6174 - val_accuracy: 0.0000e+00 - val_loss: 1.0145\n",
      "Epoch 6/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6741 - loss: 0.6236 - val_accuracy: 0.0000e+00 - val_loss: 1.0490\n",
      "Epoch 7/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6663 - loss: 0.6268 - val_accuracy: 0.0000e+00 - val_loss: 1.0880\n",
      "Epoch 8/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6702 - loss: 0.6222 - val_accuracy: 0.0000e+00 - val_loss: 1.1227\n",
      "Epoch 9/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6702 - loss: 0.6245 - val_accuracy: 0.0000e+00 - val_loss: 1.1447\n",
      "Epoch 10/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6585 - loss: 0.6342 - val_accuracy: 0.0000e+00 - val_loss: 1.1470\n",
      "Epoch 11/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7054 - loss: 0.5949 - val_accuracy: 0.0000e+00 - val_loss: 1.1352\n",
      "Epoch 12/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7405 - loss: 0.5667 - val_accuracy: 0.0000e+00 - val_loss: 1.1211\n",
      "Epoch 13/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6663 - loss: 0.6222 - val_accuracy: 0.0000e+00 - val_loss: 1.0933\n",
      "Epoch 14/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6585 - loss: 0.6268 - val_accuracy: 0.0000e+00 - val_loss: 1.0834\n",
      "Epoch 15/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6898 - loss: 0.6032 - val_accuracy: 0.0000e+00 - val_loss: 1.0918\n",
      "Epoch 16/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7093 - loss: 0.5885 - val_accuracy: 0.0000e+00 - val_loss: 1.1041\n",
      "Epoch 17/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7171 - loss: 0.5836 - val_accuracy: 0.0000e+00 - val_loss: 1.1016\n",
      "Epoch 18/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6937 - loss: 0.5974 - val_accuracy: 0.0000e+00 - val_loss: 1.0842\n",
      "Epoch 19/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6898 - loss: 0.6001 - val_accuracy: 0.0000e+00 - val_loss: 1.0719\n",
      "Epoch 20/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6702 - loss: 0.6115 - val_accuracy: 0.0000e+00 - val_loss: 1.0674\n",
      "Epoch 21/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6624 - loss: 0.6156 - val_accuracy: 0.0000e+00 - val_loss: 1.0889\n",
      "Epoch 22/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6741 - loss: 0.6086 - val_accuracy: 0.0000e+00 - val_loss: 1.1081\n",
      "Epoch 23/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6859 - loss: 0.5986 - val_accuracy: 0.0000e+00 - val_loss: 1.1241\n",
      "Epoch 24/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7249 - loss: 0.5695 - val_accuracy: 0.0000e+00 - val_loss: 1.1400\n",
      "Epoch 25/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6702 - loss: 0.6051 - val_accuracy: 0.0000e+00 - val_loss: 1.1330\n",
      "Epoch 26/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6585 - loss: 0.6153 - val_accuracy: 0.0000e+00 - val_loss: 1.1330\n",
      "Epoch 27/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7093 - loss: 0.5762 - val_accuracy: 0.0000e+00 - val_loss: 1.1435\n",
      "Epoch 28/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6663 - loss: 0.6058 - val_accuracy: 0.0000e+00 - val_loss: 1.1373\n",
      "Epoch 29/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6702 - loss: 0.6001 - val_accuracy: 0.0000e+00 - val_loss: 1.1374\n",
      "Epoch 30/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6741 - loss: 0.5922 - val_accuracy: 0.0000e+00 - val_loss: 1.1431\n",
      "Epoch 31/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7132 - loss: 0.5626 - val_accuracy: 0.0000e+00 - val_loss: 1.1456\n",
      "Epoch 32/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6741 - loss: 0.5938 - val_accuracy: 0.0000e+00 - val_loss: 1.1317\n",
      "Epoch 33/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6546 - loss: 0.6055 - val_accuracy: 0.0000e+00 - val_loss: 1.1238\n",
      "Epoch 34/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7015 - loss: 0.5610 - val_accuracy: 0.0000e+00 - val_loss: 1.1356\n",
      "Epoch 35/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6976 - loss: 0.5739 - val_accuracy: 0.0000e+00 - val_loss: 1.1269\n",
      "Epoch 36/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6898 - loss: 0.5710 - val_accuracy: 0.0000e+00 - val_loss: 1.1009\n",
      "Epoch 37/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6546 - loss: 0.6008 - val_accuracy: 0.0000e+00 - val_loss: 1.0777\n",
      "Epoch 38/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6663 - loss: 0.5896 - val_accuracy: 0.0000e+00 - val_loss: 1.0798\n",
      "Epoch 39/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6937 - loss: 0.5611 - val_accuracy: 0.0000e+00 - val_loss: 1.0935\n",
      "Epoch 40/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7015 - loss: 0.5437 - val_accuracy: 0.0000e+00 - val_loss: 1.0874\n",
      "Epoch 41/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7185 - loss: 0.5517 - val_accuracy: 0.0000e+00 - val_loss: 1.0391\n",
      "Epoch 42/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7486 - loss: 0.5563 - val_accuracy: 0.0500 - val_loss: 0.9910\n",
      "Epoch 43/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7356 - loss: 0.5565 - val_accuracy: 0.0500 - val_loss: 0.9525\n",
      "Epoch 44/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7512 - loss: 0.5450 - val_accuracy: 0.0500 - val_loss: 0.9549\n",
      "Epoch 45/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7317 - loss: 0.5471 - val_accuracy: 0.0500 - val_loss: 0.9996\n",
      "Epoch 46/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7551 - loss: 0.5431 - val_accuracy: 0.0000e+00 - val_loss: 1.0610\n",
      "Epoch 47/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7161 - loss: 0.5651 - val_accuracy: 0.0500 - val_loss: 1.0564\n",
      "Epoch 48/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7278 - loss: 0.5519 - val_accuracy: 0.0500 - val_loss: 1.0562\n",
      "Epoch 49/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7239 - loss: 0.5440 - val_accuracy: 0.0500 - val_loss: 1.0357\n",
      "Epoch 50/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7422 - loss: 0.5401 - val_accuracy: 0.0500 - val_loss: 1.0285\n",
      "Epoch 51/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7578 - loss: 0.5225 - val_accuracy: 0.0500 - val_loss: 1.0433\n",
      "Epoch 52/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7383 - loss: 0.5539 - val_accuracy: 0.0500 - val_loss: 1.0525\n",
      "Epoch 53/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7656 - loss: 0.5166 - val_accuracy: 0.0500 - val_loss: 1.0554\n",
      "Epoch 54/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7317 - loss: 0.5369 - val_accuracy: 0.0500 - val_loss: 1.0504\n",
      "Epoch 55/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7852 - loss: 0.5011 - val_accuracy: 0.0500 - val_loss: 1.0627\n",
      "Epoch 56/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7539 - loss: 0.5194 - val_accuracy: 0.1500 - val_loss: 0.9873\n",
      "Epoch 57/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7642 - loss: 0.4899 - val_accuracy: 0.1500 - val_loss: 0.9591\n",
      "Epoch 58/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7368 - loss: 0.5048 - val_accuracy: 0.2000 - val_loss: 0.9233\n",
      "Epoch 59/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7486 - loss: 0.4975 - val_accuracy: 0.2000 - val_loss: 0.9211\n",
      "Epoch 60/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7095 - loss: 0.5367 - val_accuracy: 0.2500 - val_loss: 0.9038\n",
      "Epoch 61/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7017 - loss: 0.5144 - val_accuracy: 0.1500 - val_loss: 0.9714\n",
      "Epoch 62/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7056 - loss: 0.5140 - val_accuracy: 0.1500 - val_loss: 1.0456\n",
      "Epoch 63/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7669 - loss: 0.4929 - val_accuracy: 0.0500 - val_loss: 1.0927\n",
      "Epoch 64/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7630 - loss: 0.4762 - val_accuracy: 0.1500 - val_loss: 1.0099\n",
      "Epoch 65/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7212 - loss: 0.4980 - val_accuracy: 0.3500 - val_loss: 0.8721\n",
      "Epoch 66/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7185 - loss: 0.4912 - val_accuracy: 0.3500 - val_loss: 0.8416\n",
      "Epoch 67/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6768 - loss: 0.5175 - val_accuracy: 0.3000 - val_loss: 0.9007\n",
      "Epoch 68/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7278 - loss: 0.5014 - val_accuracy: 0.1500 - val_loss: 0.9815\n",
      "Epoch 69/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7017 - loss: 0.4987 - val_accuracy: 0.1500 - val_loss: 1.0686\n",
      "Epoch 70/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7395 - loss: 0.4794 - val_accuracy: 0.0500 - val_loss: 1.1424\n",
      "Epoch 71/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7227 - loss: 0.5068 - val_accuracy: 0.1500 - val_loss: 1.0951\n",
      "Epoch 72/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7278 - loss: 0.5074 - val_accuracy: 0.1500 - val_loss: 0.9886\n",
      "Epoch 73/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7068 - loss: 0.4884 - val_accuracy: 0.3500 - val_loss: 0.8773\n",
      "Epoch 74/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7590 - loss: 0.4747 - val_accuracy: 0.3500 - val_loss: 0.8640\n",
      "Epoch 75/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7356 - loss: 0.4882 - val_accuracy: 0.3500 - val_loss: 0.8722\n",
      "Epoch 76/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7278 - loss: 0.4927 - val_accuracy: 0.2000 - val_loss: 0.9605\n",
      "Epoch 77/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7173 - loss: 0.4833 - val_accuracy: 0.1500 - val_loss: 1.0403\n",
      "Epoch 78/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7500 - loss: 0.4721 - val_accuracy: 0.1500 - val_loss: 1.0460\n",
      "Epoch 79/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7630 - loss: 0.4518 - val_accuracy: 0.2000 - val_loss: 1.0066\n",
      "Epoch 80/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7251 - loss: 0.4657 - val_accuracy: 0.3500 - val_loss: 0.8565\n",
      "Epoch 81/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7317 - loss: 0.4709 - val_accuracy: 0.4000 - val_loss: 0.8468\n",
      "Epoch 82/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7539 - loss: 0.4529 - val_accuracy: 0.2000 - val_loss: 0.9608\n",
      "Epoch 83/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7708 - loss: 0.4390 - val_accuracy: 0.2000 - val_loss: 0.9608\n",
      "Epoch 84/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7551 - loss: 0.4214 - val_accuracy: 0.4000 - val_loss: 0.8461\n",
      "Epoch 85/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7671 - loss: 0.4791 - val_accuracy: 0.4500 - val_loss: 0.7524\n",
      "Epoch 86/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7566 - loss: 0.4507 - val_accuracy: 0.4000 - val_loss: 0.8174\n",
      "Epoch 87/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7932 - loss: 0.4506 - val_accuracy: 0.2500 - val_loss: 0.8926\n",
      "Epoch 88/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7422 - loss: 0.4516 - val_accuracy: 0.2000 - val_loss: 1.0236\n",
      "Epoch 89/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7422 - loss: 0.4677 - val_accuracy: 0.2000 - val_loss: 1.0197\n",
      "Epoch 90/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7305 - loss: 0.4654 - val_accuracy: 0.2000 - val_loss: 0.9465\n",
      "Epoch 91/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7866 - loss: 0.4322 - val_accuracy: 0.4000 - val_loss: 0.8684\n",
      "Epoch 92/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7866 - loss: 0.4341 - val_accuracy: 0.4000 - val_loss: 0.8201\n",
      "Epoch 93/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8076 - loss: 0.4270 - val_accuracy: 0.4000 - val_loss: 0.8182\n",
      "Epoch 94/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7593 - loss: 0.4640 - val_accuracy: 0.3000 - val_loss: 0.8686\n",
      "Epoch 95/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8010 - loss: 0.4322 - val_accuracy: 0.3000 - val_loss: 0.8799\n",
      "Epoch 96/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8049 - loss: 0.4076 - val_accuracy: 0.3500 - val_loss: 0.8548\n",
      "Epoch 97/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7893 - loss: 0.4317 - val_accuracy: 0.4000 - val_loss: 0.7757\n",
      "Epoch 98/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8012 - loss: 0.4434 - val_accuracy: 0.5000 - val_loss: 0.7207\n",
      "Epoch 99/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8690 - loss: 0.4049 - val_accuracy: 0.4000 - val_loss: 0.7717\n",
      "Epoch 100/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8141 - loss: 0.4336 - val_accuracy: 0.3000 - val_loss: 0.8433\n",
      "Epoch 101/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8102 - loss: 0.4071 - val_accuracy: 0.4000 - val_loss: 0.8283\n",
      "Epoch 102/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8102 - loss: 0.4152 - val_accuracy: 0.3500 - val_loss: 0.8266\n",
      "Epoch 103/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7985 - loss: 0.4337 - val_accuracy: 0.3500 - val_loss: 0.8261\n",
      "Epoch 104/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8102 - loss: 0.3801 - val_accuracy: 0.3000 - val_loss: 0.8267\n",
      "Epoch 105/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8468 - loss: 0.4020 - val_accuracy: 0.4000 - val_loss: 0.7956\n",
      "Epoch 106/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8285 - loss: 0.4069 - val_accuracy: 0.4000 - val_loss: 0.7781\n",
      "Epoch 107/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8546 - loss: 0.3683 - val_accuracy: 0.4000 - val_loss: 0.7294\n",
      "Epoch 108/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8507 - loss: 0.3956 - val_accuracy: 0.5500 - val_loss: 0.6915\n",
      "Epoch 109/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8586 - loss: 0.3714 - val_accuracy: 0.4000 - val_loss: 0.7592\n",
      "Epoch 110/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8405 - loss: 0.4110 - val_accuracy: 0.3500 - val_loss: 0.7862\n",
      "Epoch 111/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8598 - loss: 0.3702 - val_accuracy: 0.3500 - val_loss: 0.8270\n",
      "Epoch 112/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8586 - loss: 0.3681 - val_accuracy: 0.4000 - val_loss: 0.7324\n",
      "Epoch 113/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8639 - loss: 0.3766 - val_accuracy: 0.7500 - val_loss: 0.6330\n",
      "Epoch 114/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8039 - loss: 0.4054 - val_accuracy: 0.6500 - val_loss: 0.6701\n",
      "Epoch 115/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8678 - loss: 0.3743 - val_accuracy: 0.3500 - val_loss: 0.7657\n",
      "Epoch 116/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8456 - loss: 0.3870 - val_accuracy: 0.3500 - val_loss: 0.7884\n",
      "Epoch 117/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8456 - loss: 0.3716 - val_accuracy: 0.6500 - val_loss: 0.6716\n",
      "Epoch 118/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8873 - loss: 0.3546 - val_accuracy: 0.7000 - val_loss: 0.6382\n",
      "Epoch 119/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8600 - loss: 0.3839 - val_accuracy: 0.7500 - val_loss: 0.5954\n",
      "Epoch 120/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8808 - loss: 0.3384 - val_accuracy: 0.6500 - val_loss: 0.6610\n",
      "Epoch 121/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9017 - loss: 0.3456 - val_accuracy: 0.6500 - val_loss: 0.6741\n",
      "Epoch 122/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8834 - loss: 0.3416 - val_accuracy: 0.7000 - val_loss: 0.6370\n",
      "Epoch 123/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8900 - loss: 0.3489 - val_accuracy: 0.7500 - val_loss: 0.5633\n",
      "Epoch 124/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8495 - loss: 0.3517 - val_accuracy: 0.7500 - val_loss: 0.5557\n",
      "Epoch 125/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8573 - loss: 0.3435 - val_accuracy: 0.7500 - val_loss: 0.6012\n",
      "Epoch 126/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8678 - loss: 0.3442 - val_accuracy: 0.4500 - val_loss: 0.7278\n",
      "Epoch 127/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8756 - loss: 0.3241 - val_accuracy: 0.4500 - val_loss: 0.7299\n",
      "Epoch 128/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9030 - loss: 0.3136 - val_accuracy: 0.7500 - val_loss: 0.5210\n",
      "Epoch 129/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8810 - loss: 0.3331 - val_accuracy: 0.8500 - val_loss: 0.4223\n",
      "Epoch 130/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8927 - loss: 0.3464 - val_accuracy: 0.7500 - val_loss: 0.5191\n",
      "Epoch 131/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8861 - loss: 0.3210 - val_accuracy: 0.5000 - val_loss: 0.6793\n",
      "Epoch 132/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8627 - loss: 0.3589 - val_accuracy: 0.4500 - val_loss: 0.7502\n",
      "Epoch 133/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8600 - loss: 0.3485 - val_accuracy: 0.5000 - val_loss: 0.6880\n",
      "Epoch 134/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8900 - loss: 0.3219 - val_accuracy: 0.7500 - val_loss: 0.5972\n",
      "Epoch 135/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8861 - loss: 0.3097 - val_accuracy: 0.7500 - val_loss: 0.5375\n",
      "Epoch 136/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8627 - loss: 0.3230 - val_accuracy: 0.7500 - val_loss: 0.5229\n",
      "Epoch 137/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9110 - loss: 0.3114 - val_accuracy: 0.7500 - val_loss: 0.5599\n",
      "Epoch 138/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8822 - loss: 0.3009 - val_accuracy: 0.7000 - val_loss: 0.5929\n",
      "Epoch 139/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8744 - loss: 0.3004 - val_accuracy: 0.8000 - val_loss: 0.5180\n",
      "Epoch 140/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9344 - loss: 0.2727 - val_accuracy: 0.8000 - val_loss: 0.4796\n",
      "Epoch 141/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9293 - loss: 0.3041 - val_accuracy: 0.8000 - val_loss: 0.4626\n",
      "Epoch 142/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9437 - loss: 0.2805 - val_accuracy: 0.8000 - val_loss: 0.4630\n",
      "Epoch 143/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9163 - loss: 0.2971 - val_accuracy: 0.8000 - val_loss: 0.4892\n",
      "Epoch 144/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9188 - loss: 0.2718 - val_accuracy: 0.8000 - val_loss: 0.4912\n",
      "Epoch 145/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9215 - loss: 0.2722 - val_accuracy: 0.8500 - val_loss: 0.4168\n",
      "Epoch 146/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9437 - loss: 0.2813 - val_accuracy: 0.8000 - val_loss: 0.5001\n",
      "Epoch 147/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9254 - loss: 0.2639 - val_accuracy: 0.8000 - val_loss: 0.4974\n",
      "Epoch 148/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9215 - loss: 0.2695 - val_accuracy: 0.8000 - val_loss: 0.4632\n",
      "Epoch 149/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9058 - loss: 0.2823 - val_accuracy: 1.0000 - val_loss: 0.3744\n",
      "Epoch 150/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9502 - loss: 0.2766 - val_accuracy: 0.9000 - val_loss: 0.4295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Define a simple Sequential model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(34,)),  # Adjust input_dim to match your data\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Use 'softmax' for multi-class classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',  # Use 'categorical_crossentropy' for multi-class classification\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_df, y_df, epochs=150, validation_split=0.2)  # Adjust epochs as needed\n",
    "\n",
    "# Save the model in a format suitable for TensorFlow Lite\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def landmarks_from_squating_images(input_dir,X,Y):\n",
    "    for file in os.listdir(input_dir):\n",
    "        img_path = os.path.join(input_dir, file)\n",
    "        \n",
    "        # Check if the file is an image\n",
    "        if os.path.isfile(img_path) and file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "            img = Image.open(img_path)\n",
    "            img = tf.image.resize_with_pad(np.expand_dims(img,axis=0), 192, 192)\n",
    "            input_image = tf.cast(img, dtype=tf.float32)\n",
    "\n",
    "            # Setup input and output\n",
    "            input_details = interpreter.get_input_details()\n",
    "            output_details = interpreter.get_output_details()\n",
    "\n",
    "            #make predictions\n",
    "            interpreter.set_tensor(input_details[0]['index'], np.array(input_image))\n",
    "            interpreter.invoke()\n",
    "            keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
    "            landmarks = keypoints_with_scores\n",
    "\n",
    "            X.append(landmarks)\n",
    "            Y.append(1)\n",
    "    return X,Y\n",
    "\n",
    "def image_prediction(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    img = tf.image.resize_with_pad(np.expand_dims(img,axis=0), 192, 192)\n",
    "    input_image = tf.cast(img, dtype=tf.float32)\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    #make predictions\n",
    "    interpreter.set_tensor(input_details[0]['index'], np.array(input_image))\n",
    "    interpreter.invoke()\n",
    "    keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
    "    landmarks = keypoints_with_scores\n",
    "    return landmarks[0][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X,test_Y = landmarks_from_squating_images('test images/sit',[],[])\n",
    "test_X,test_Y = landmarks_from_standing_images('test images/stand',test_X,test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        1_x       2_x       3_x       4_x       5_x       6_x       7_x  \\\n",
      "0  0.158883  0.129680  0.128846  0.141852  0.142761  0.247204  0.251513   \n",
      "1  0.422304  0.404254  0.404367  0.414068  0.414410  0.490574  0.495341   \n",
      "2  0.396914  0.384297  0.385278  0.400497  0.403513  0.484471  0.460770   \n",
      "3  0.547260  0.536314  0.538534  0.546335  0.548772  0.611116  0.613522   \n",
      "4  0.148019  0.126063  0.125789  0.158340  0.144929  0.266824  0.281123   \n",
      "5  0.092995  0.069636  0.069766  0.071858  0.072698  0.131020  0.142498   \n",
      "6  0.179094  0.169309  0.169256  0.192265  0.193769  0.293402  0.265297   \n",
      "7  0.355024  0.343350  0.343809  0.354249  0.354406  0.420247  0.419390   \n",
      "8  0.122018  0.101319  0.100482  0.117442  0.109174  0.195384  0.211573   \n",
      "9  0.306403  0.289890  0.288576  0.280730  0.278254  0.337062  0.335010   \n",
      "\n",
      "        8_x       9_x      10_x  ...       8_y       9_y      10_y      11_y  \\\n",
      "0  0.534532  0.501605  0.401277  ...  0.269241  0.339590  0.231762  0.254074   \n",
      "1  0.625781  0.643805  0.515322  ...  0.604323  0.624982  0.652770  0.663743   \n",
      "2  0.598431  0.578372  0.522327  ...  0.660944  0.469270  0.634794  0.581783   \n",
      "3  0.688228  0.697918  0.607606  ...  0.413410  0.662110  0.453288  0.695115   \n",
      "4  0.380627  0.409647  0.267337  ...  0.455894  0.206747  0.345628  0.276372   \n",
      "5  0.287329  0.293203  0.420206  ...  0.507669  0.508359  0.499327  0.501252   \n",
      "6  0.389714  0.372048  0.429880  ...  0.628719  0.394480  0.697744  0.444793   \n",
      "7  0.492874  0.490980  0.444824  ...  0.388984  0.642489  0.402189  0.645673   \n",
      "8  0.267429  0.328119  0.251469  ...  0.556874  0.325422  0.445378  0.388558   \n",
      "9  0.461710  0.465893  0.568288  ...  0.483583  0.489451  0.501375  0.497163   \n",
      "\n",
      "       12_y      13_y      14_y      15_y      16_y      17_y  \n",
      "0  0.690922  0.672519  0.294365  0.307713  0.534263  0.558230  \n",
      "1  0.345256  0.348218  0.538866  0.572163  0.442801  0.463322  \n",
      "2  0.463619  0.360924  0.566067  0.457149  0.498338  0.410601  \n",
      "3  0.475448  0.577770  0.428070  0.652307  0.429497  0.616923  \n",
      "4  0.691065  0.525777  0.528453  0.304899  0.644951  0.399818  \n",
      "5  0.500854  0.503749  0.480244  0.525200  0.513232  0.553748  \n",
      "6  0.511080  0.419761  0.527694  0.446920  0.484829  0.407724  \n",
      "7  0.474317  0.549404  0.443026  0.589325  0.430568  0.612737  \n",
      "8  0.558499  0.477439  0.569109  0.450073  0.616672  0.437894  \n",
      "9  0.472780  0.479109  0.477327  0.481199  0.462648  0.462623  \n",
      "\n",
      "[10 rows x 34 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2001l\\AppData\\Local\\Temp\\ipykernel_56372\\1551579132.py:14: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_test = pd.concat([df_test, pd.DataFrame([new_row])], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "test_columns = [cols for cols in columns if cols != 'Y']\n",
    "\n",
    "df_test = pd.DataFrame(columns=test_columns)\n",
    "\n",
    "for entry in test_X:\n",
    "    new_row = {col: None for col in test_columns}\n",
    "    x = 0\n",
    "    for x in range(1,18):\n",
    "        xName = str(x) + '_x'\n",
    "        yName = str(x) + '_y'\n",
    "        new_row[xName] = entry[0][0][x-1][0]\n",
    "        new_row[yName] = entry[0][0][x-1][1]\n",
    "        x = x+1\n",
    "    df_test = pd.concat([df_test, pd.DataFrame([new_row])], ignore_index=True)\n",
    "print(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8971357 ],\n",
       "       [0.88019645],\n",
       "       [0.7610879 ],\n",
       "       [0.95123905],\n",
       "       [0.63445026],\n",
       "       [0.02163909],\n",
       "       [0.12390265],\n",
       "       [0.44931784],\n",
       "       [0.06793758],\n",
       "       [0.11182853]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
