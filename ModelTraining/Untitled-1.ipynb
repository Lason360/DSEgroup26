{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path = r'movenet.tflite')\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flipping Training Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flipped and saved: 0_960x540.jpg\n",
      "Flipped and saved: 0_960x540.jpg_flipped.jpg\n",
      "Flipped and saved: 0_960x540.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: 0_960x540.jpg_flipped.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: 2potEB8qNMABrHqjfWIpOOvbyAbsNmbLb7_SAM9jEQE.jpg\n",
      "Flipped and saved: 2potEB8qNMABrHqjfWIpOOvbyAbsNmbLb7_SAM9jEQE.jpg_flipped.jpg\n",
      "Flipped and saved: 2potEB8qNMABrHqjfWIpOOvbyAbsNmbLb7_SAM9jEQE.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: 2potEB8qNMABrHqjfWIpOOvbyAbsNmbLb7_SAM9jEQE.jpg_flipped.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: 362-1.jpg\n",
      "Flipped and saved: 362-1.jpg_flipped.jpg\n",
      "Flipped and saved: 362-1.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: 362-1.jpg_flipped.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: 524.jpg\n",
      "Flipped and saved: 524.jpg_flipped.jpg\n",
      "Flipped and saved: 524.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: 524.jpg_flipped.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: 6181ae52a3e0153fba71466fc28cb1fc595b4647_720.jpg\n",
      "Flipped and saved: 6181ae52a3e0153fba71466fc28cb1fc595b4647_720.jpg_flipped.jpg\n",
      "Flipped and saved: 6181ae52a3e0153fba71466fc28cb1fc595b4647_720.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: 6181ae52a3e0153fba71466fc28cb1fc595b4647_720.jpg_flipped.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: Barbell-Back-Squat.jpg\n",
      "Flipped and saved: Barbell-Back-Squat.jpg_flipped.jpg\n",
      "Flipped and saved: Barbell-Back-Squat.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: Barbell-Back-Squat.jpg_flipped.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: Blog-Image-7-STANDING-AB-EXERCISES-THAT-WILL-STRENGTHEN-YOUR-CORE.jpg\n",
      "Flipped and saved: Blog-Image-7-STANDING-AB-EXERCISES-THAT-WILL-STRENGTHEN-YOUR-CORE.jpg_flipped.jpg\n",
      "Flipped and saved: Blog-Image-7-STANDING-AB-EXERCISES-THAT-WILL-STRENGTHEN-YOUR-CORE.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: Blog-Image-7-STANDING-AB-EXERCISES-THAT-WILL-STRENGTHEN-YOUR-CORE.jpg_flipped.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: f2-barbell-back-squat.jpg\n",
      "Flipped and saved: f2-barbell-back-squat.jpg_flipped.jpg\n",
      "Flipped and saved: f2-barbell-back-squat.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: f2-barbell-back-squat.jpg_flipped.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: hq720 (1).jpg\n",
      "Flipped and saved: hq720 (1).jpg_flipped.jpg\n",
      "Flipped and saved: hq720 (1).jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: hq720 (1).jpg_flipped.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: hq720.jpg\n",
      "Flipped and saved: hq720.jpg_flipped.jpg\n",
      "Flipped and saved: hq720.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: hq720.jpg_flipped.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: man-1203899_1280.jpg\n",
      "Flipped and saved: man-1203899_1280.jpg_flipped.jpg\n",
      "Flipped and saved: man-1203899_1280.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: man-1203899_1280.jpg_flipped.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 181414.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 181414.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 181414.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 181414.jpg_flipped.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 181458.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 181458.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 181458.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 181458.jpg_flipped.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 181607.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 181607.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 181607.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 181607.jpg_flipped.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 181644.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 181644.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 181644.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 181644.jpg_flipped.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 181720.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 181720.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 181720.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 181720.jpg_flipped.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 182206.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 182206.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 182206.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 182206.jpg_flipped.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 182443.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 182443.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 182443.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 182443.jpg_flipped.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 182809.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 182809.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 182809.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 182809.jpg_flipped.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 182942.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 182942.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 182942.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 182942.jpg_flipped.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 183554.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 183554.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 183554.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 183554.jpg_flipped.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 183631.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 183631.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 183631.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 183631.jpg_flipped.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 183709.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 183709.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 183709.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 183709.jpg_flipped.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 183735.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 183735.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 183735.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: Screenshot 2024-08-18 183735.jpg_flipped.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: Squat.StartPosition.GastownPhysioPilates.jpg\n",
      "Flipped and saved: Squat.StartPosition.GastownPhysioPilates.jpg_flipped.jpg\n",
      "Flipped and saved: Squat.StartPosition.GastownPhysioPilates.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: Squat.StartPosition.GastownPhysioPilates.jpg_flipped.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: standing-position.jpg\n",
      "Flipped and saved: standing-position.jpg_flipped.jpg\n",
      "Flipped and saved: standing-position.jpg_flipped.jpg_flipped.jpg\n",
      "Flipped and saved: standing-position.jpg_flipped.jpg_flipped.jpg_flipped.jpg\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def flip_images(input_dir, output_dir):\n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Loop through all the files in the input directory\n",
    "    for filename in os.listdir(input_dir):\n",
    "        # Construct the full file path\n",
    "        img_path = os.path.join(input_dir, filename)\n",
    "        \n",
    "        # Check if the file is an image\n",
    "        if os.path.isfile(img_path) and filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "            # Open the image\n",
    "            img = Image.open(img_path)\n",
    "            \n",
    "            # Flip the image horizontally\n",
    "            flipped_img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            \n",
    "            # Save the flipped image to the output directory\n",
    "            flipped_img.save(os.path.join(output_dir, filename+'_flipped.jpg'))\n",
    "            print(f\"Flipped and saved: {filename}\")\n",
    "\n",
    "# Example usage:\n",
    "input_directory = r'C:\\Users\\2001l\\OneDrive\\Desktop\\squat\\upPosition'\n",
    "output_directory = r'C:\\Users\\2001l\\OneDrive\\Desktop\\squat\\upPosition'\n",
    "\n",
    "flip_images(input_directory, output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe of landmarks of the training Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def landmarks_from_standing_images(input_dir,X,Y):\n",
    "    for file in os.listdir(input_dir):\n",
    "        img_path = os.path.join(input_dir, file)\n",
    "        \n",
    "        # Check if the file is an image\n",
    "        if os.path.isfile(img_path) and file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "            img = Image.open(img_path)\n",
    "            img = tf.image.resize_with_pad(np.expand_dims(img,axis=0), 192, 192)\n",
    "            input_image = tf.cast(img, dtype=tf.float32)\n",
    "\n",
    "            # Setup input and output\n",
    "            input_details = interpreter.get_input_details()\n",
    "            output_details = interpreter.get_output_details()\n",
    "\n",
    "            #make predictions\n",
    "            interpreter.set_tensor(input_details[0]['index'], np.array(input_image))\n",
    "            interpreter.invoke()\n",
    "            keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
    "            landmarks = keypoints_with_scores\n",
    "\n",
    "            X.append(landmarks)\n",
    "            Y.append(0)\n",
    "    return X,Y\n",
    "\n",
    "def landmarks_from_squating_images(input_dir,X,Y):\n",
    "    for file in os.listdir(input_dir):\n",
    "        img_path = os.path.join(input_dir, file)\n",
    "        \n",
    "        # Check if the file is an image\n",
    "        if os.path.isfile(img_path) and file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "            img = Image.open(img_path)\n",
    "            img = tf.image.resize_with_pad(np.expand_dims(img,axis=0), 192, 192)\n",
    "            input_image = tf.cast(img, dtype=tf.float32)\n",
    "\n",
    "            # Setup input and output\n",
    "            input_details = interpreter.get_input_details()\n",
    "            output_details = interpreter.get_output_details()\n",
    "\n",
    "            #make predictions\n",
    "            interpreter.set_tensor(input_details[0]['index'], np.array(input_image))\n",
    "            interpreter.invoke()\n",
    "            keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
    "            landmarks = keypoints_with_scores\n",
    "\n",
    "            X.append(landmarks)\n",
    "            Y.append(1)\n",
    "    return X,Y\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = landmarks_from_standing_images(r'upPosition',[],[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = landmarks_from_squating_images(r'downPosition',X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [1_x, 2_x, 3_x, 4_x, 5_x, 6_x, 7_x, 8_x, 9_x, 10_x, 11_x, 12_x, 13_x, 14_x, 15_x, 16_x, 17_x, 1_y, 2_y, 3_y, 4_y, 5_y, 6_y, 7_y, 8_y, 9_y, 10_y, 11_y, 12_y, 13_y, 14_y, 15_y, 16_y, 17_y, Y]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Generate column names\n",
    "columns = [f\"{i}_x\" for i in range(1, 18)] + [f\"{i}_y\" for i in range(1, 18)] + ['Y']\n",
    "\n",
    "# Create an empty DataFrame with these columns\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Display the DataFrame to verify\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2001l\\AppData\\Local\\Temp\\ipykernel_46272\\1965758734.py:15: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "y = 0\n",
    "for entry in X:\n",
    "    new_row = {col: None for col in columns}\n",
    "    x = 0\n",
    "    for x in range(1,18):\n",
    "        xName = str(x) + '_x'\n",
    "        yName = str(x) + '_y'\n",
    "        new_row[xName] = entry[0][0][x-1][0]\n",
    "        new_row[yName] = entry[0][0][x-1][1]\n",
    "        x = x+1\n",
    "    new_row['Y'] = Y[y]\n",
    "    y = y+1\n",
    "    df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1_x</th>\n",
       "      <th>2_x</th>\n",
       "      <th>3_x</th>\n",
       "      <th>4_x</th>\n",
       "      <th>5_x</th>\n",
       "      <th>6_x</th>\n",
       "      <th>7_x</th>\n",
       "      <th>8_x</th>\n",
       "      <th>9_x</th>\n",
       "      <th>10_x</th>\n",
       "      <th>...</th>\n",
       "      <th>9_y</th>\n",
       "      <th>10_y</th>\n",
       "      <th>11_y</th>\n",
       "      <th>12_y</th>\n",
       "      <th>13_y</th>\n",
       "      <th>14_y</th>\n",
       "      <th>15_y</th>\n",
       "      <th>16_y</th>\n",
       "      <th>17_y</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.332451</td>\n",
       "      <td>0.327286</td>\n",
       "      <td>0.328188</td>\n",
       "      <td>0.335584</td>\n",
       "      <td>0.337321</td>\n",
       "      <td>0.376591</td>\n",
       "      <td>0.380630</td>\n",
       "      <td>0.395435</td>\n",
       "      <td>0.405161</td>\n",
       "      <td>0.371132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.455058</td>\n",
       "      <td>0.564492</td>\n",
       "      <td>0.452833</td>\n",
       "      <td>0.538660</td>\n",
       "      <td>0.492199</td>\n",
       "      <td>0.552743</td>\n",
       "      <td>0.490987</td>\n",
       "      <td>0.564235</td>\n",
       "      <td>0.489883</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.337427</td>\n",
       "      <td>0.330524</td>\n",
       "      <td>0.334669</td>\n",
       "      <td>0.338237</td>\n",
       "      <td>0.339605</td>\n",
       "      <td>0.375162</td>\n",
       "      <td>0.382447</td>\n",
       "      <td>0.390089</td>\n",
       "      <td>0.408159</td>\n",
       "      <td>0.346712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.430101</td>\n",
       "      <td>0.523434</td>\n",
       "      <td>0.440300</td>\n",
       "      <td>0.505220</td>\n",
       "      <td>0.463085</td>\n",
       "      <td>0.507366</td>\n",
       "      <td>0.448752</td>\n",
       "      <td>0.511667</td>\n",
       "      <td>0.434354</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.248382</td>\n",
       "      <td>0.234584</td>\n",
       "      <td>0.236951</td>\n",
       "      <td>0.245007</td>\n",
       "      <td>0.244894</td>\n",
       "      <td>0.292224</td>\n",
       "      <td>0.277356</td>\n",
       "      <td>0.367411</td>\n",
       "      <td>0.338310</td>\n",
       "      <td>0.382827</td>\n",
       "      <td>...</td>\n",
       "      <td>0.713014</td>\n",
       "      <td>0.429093</td>\n",
       "      <td>0.720514</td>\n",
       "      <td>0.516982</td>\n",
       "      <td>0.610946</td>\n",
       "      <td>0.498965</td>\n",
       "      <td>0.643391</td>\n",
       "      <td>0.495000</td>\n",
       "      <td>0.678543</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.252336</td>\n",
       "      <td>0.240113</td>\n",
       "      <td>0.242199</td>\n",
       "      <td>0.246664</td>\n",
       "      <td>0.248041</td>\n",
       "      <td>0.277887</td>\n",
       "      <td>0.293914</td>\n",
       "      <td>0.341917</td>\n",
       "      <td>0.363804</td>\n",
       "      <td>0.387278</td>\n",
       "      <td>...</td>\n",
       "      <td>0.570867</td>\n",
       "      <td>0.278646</td>\n",
       "      <td>0.568406</td>\n",
       "      <td>0.385382</td>\n",
       "      <td>0.474913</td>\n",
       "      <td>0.346625</td>\n",
       "      <td>0.487613</td>\n",
       "      <td>0.319134</td>\n",
       "      <td>0.507263</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.241581</td>\n",
       "      <td>0.232052</td>\n",
       "      <td>0.231702</td>\n",
       "      <td>0.251969</td>\n",
       "      <td>0.244944</td>\n",
       "      <td>0.315338</td>\n",
       "      <td>0.316320</td>\n",
       "      <td>0.416093</td>\n",
       "      <td>0.418072</td>\n",
       "      <td>0.356919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.445377</td>\n",
       "      <td>0.497518</td>\n",
       "      <td>0.468692</td>\n",
       "      <td>0.557133</td>\n",
       "      <td>0.482137</td>\n",
       "      <td>0.591745</td>\n",
       "      <td>0.482272</td>\n",
       "      <td>0.614701</td>\n",
       "      <td>0.470514</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.269335</td>\n",
       "      <td>0.255440</td>\n",
       "      <td>0.262633</td>\n",
       "      <td>0.248578</td>\n",
       "      <td>0.264382</td>\n",
       "      <td>0.315988</td>\n",
       "      <td>0.331319</td>\n",
       "      <td>0.370942</td>\n",
       "      <td>0.472705</td>\n",
       "      <td>0.351692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.424774</td>\n",
       "      <td>0.252384</td>\n",
       "      <td>0.563788</td>\n",
       "      <td>0.183186</td>\n",
       "      <td>0.268853</td>\n",
       "      <td>0.527715</td>\n",
       "      <td>0.553091</td>\n",
       "      <td>0.381434</td>\n",
       "      <td>0.422276</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.152496</td>\n",
       "      <td>0.131350</td>\n",
       "      <td>0.130717</td>\n",
       "      <td>0.147583</td>\n",
       "      <td>0.143705</td>\n",
       "      <td>0.248949</td>\n",
       "      <td>0.246971</td>\n",
       "      <td>0.266599</td>\n",
       "      <td>0.254043</td>\n",
       "      <td>0.241453</td>\n",
       "      <td>...</td>\n",
       "      <td>0.304535</td>\n",
       "      <td>0.161116</td>\n",
       "      <td>0.155328</td>\n",
       "      <td>0.733582</td>\n",
       "      <td>0.631184</td>\n",
       "      <td>0.520522</td>\n",
       "      <td>0.423227</td>\n",
       "      <td>0.661199</td>\n",
       "      <td>0.525378</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.157098</td>\n",
       "      <td>0.132782</td>\n",
       "      <td>0.134116</td>\n",
       "      <td>0.142868</td>\n",
       "      <td>0.145697</td>\n",
       "      <td>0.240384</td>\n",
       "      <td>0.252563</td>\n",
       "      <td>0.251068</td>\n",
       "      <td>0.265584</td>\n",
       "      <td>0.243522</td>\n",
       "      <td>...</td>\n",
       "      <td>0.660548</td>\n",
       "      <td>0.841031</td>\n",
       "      <td>0.831572</td>\n",
       "      <td>0.380024</td>\n",
       "      <td>0.262156</td>\n",
       "      <td>0.575911</td>\n",
       "      <td>0.483287</td>\n",
       "      <td>0.470470</td>\n",
       "      <td>0.330142</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.291138</td>\n",
       "      <td>0.281100</td>\n",
       "      <td>0.280294</td>\n",
       "      <td>0.301488</td>\n",
       "      <td>0.296908</td>\n",
       "      <td>0.372551</td>\n",
       "      <td>0.364195</td>\n",
       "      <td>0.506963</td>\n",
       "      <td>0.483484</td>\n",
       "      <td>0.417078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.579118</td>\n",
       "      <td>0.587216</td>\n",
       "      <td>0.574139</td>\n",
       "      <td>0.729981</td>\n",
       "      <td>0.651935</td>\n",
       "      <td>0.581281</td>\n",
       "      <td>0.571586</td>\n",
       "      <td>0.655428</td>\n",
       "      <td>0.641182</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.287423</td>\n",
       "      <td>0.276282</td>\n",
       "      <td>0.279109</td>\n",
       "      <td>0.294811</td>\n",
       "      <td>0.299027</td>\n",
       "      <td>0.369762</td>\n",
       "      <td>0.377475</td>\n",
       "      <td>0.491012</td>\n",
       "      <td>0.502300</td>\n",
       "      <td>0.388003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.417008</td>\n",
       "      <td>0.412433</td>\n",
       "      <td>0.400359</td>\n",
       "      <td>0.338844</td>\n",
       "      <td>0.260979</td>\n",
       "      <td>0.428291</td>\n",
       "      <td>0.424024</td>\n",
       "      <td>0.353666</td>\n",
       "      <td>0.351520</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         1_x       2_x       3_x       4_x       5_x       6_x       7_x  \\\n",
       "0   0.332451  0.327286  0.328188  0.335584  0.337321  0.376591  0.380630   \n",
       "1   0.337427  0.330524  0.334669  0.338237  0.339605  0.375162  0.382447   \n",
       "2   0.248382  0.234584  0.236951  0.245007  0.244894  0.292224  0.277356   \n",
       "3   0.252336  0.240113  0.242199  0.246664  0.248041  0.277887  0.293914   \n",
       "4   0.241581  0.232052  0.231702  0.251969  0.244944  0.315338  0.316320   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "91  0.269335  0.255440  0.262633  0.248578  0.264382  0.315988  0.331319   \n",
       "92  0.152496  0.131350  0.130717  0.147583  0.143705  0.248949  0.246971   \n",
       "93  0.157098  0.132782  0.134116  0.142868  0.145697  0.240384  0.252563   \n",
       "94  0.291138  0.281100  0.280294  0.301488  0.296908  0.372551  0.364195   \n",
       "95  0.287423  0.276282  0.279109  0.294811  0.299027  0.369762  0.377475   \n",
       "\n",
       "         8_x       9_x      10_x  ...       9_y      10_y      11_y      12_y  \\\n",
       "0   0.395435  0.405161  0.371132  ...  0.455058  0.564492  0.452833  0.538660   \n",
       "1   0.390089  0.408159  0.346712  ...  0.430101  0.523434  0.440300  0.505220   \n",
       "2   0.367411  0.338310  0.382827  ...  0.713014  0.429093  0.720514  0.516982   \n",
       "3   0.341917  0.363804  0.387278  ...  0.570867  0.278646  0.568406  0.385382   \n",
       "4   0.416093  0.418072  0.356919  ...  0.445377  0.497518  0.468692  0.557133   \n",
       "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
       "91  0.370942  0.472705  0.351692  ...  0.424774  0.252384  0.563788  0.183186   \n",
       "92  0.266599  0.254043  0.241453  ...  0.304535  0.161116  0.155328  0.733582   \n",
       "93  0.251068  0.265584  0.243522  ...  0.660548  0.841031  0.831572  0.380024   \n",
       "94  0.506963  0.483484  0.417078  ...  0.579118  0.587216  0.574139  0.729981   \n",
       "95  0.491012  0.502300  0.388003  ...  0.417008  0.412433  0.400359  0.338844   \n",
       "\n",
       "        13_y      14_y      15_y      16_y      17_y  Y  \n",
       "0   0.492199  0.552743  0.490987  0.564235  0.489883  0  \n",
       "1   0.463085  0.507366  0.448752  0.511667  0.434354  0  \n",
       "2   0.610946  0.498965  0.643391  0.495000  0.678543  0  \n",
       "3   0.474913  0.346625  0.487613  0.319134  0.507263  0  \n",
       "4   0.482137  0.591745  0.482272  0.614701  0.470514  0  \n",
       "..       ...       ...       ...       ...       ... ..  \n",
       "91  0.268853  0.527715  0.553091  0.381434  0.422276  1  \n",
       "92  0.631184  0.520522  0.423227  0.661199  0.525378  1  \n",
       "93  0.262156  0.575911  0.483287  0.470470  0.330142  1  \n",
       "94  0.651935  0.581281  0.571586  0.655428  0.641182  1  \n",
       "95  0.260979  0.428291  0.424024  0.353666  0.351520  1  \n",
       "\n",
       "[96 rows x 35 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('squatTrainSet.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1_x</th>\n",
       "      <th>2_x</th>\n",
       "      <th>3_x</th>\n",
       "      <th>4_x</th>\n",
       "      <th>5_x</th>\n",
       "      <th>6_x</th>\n",
       "      <th>7_x</th>\n",
       "      <th>8_x</th>\n",
       "      <th>9_x</th>\n",
       "      <th>10_x</th>\n",
       "      <th>...</th>\n",
       "      <th>9_y</th>\n",
       "      <th>10_y</th>\n",
       "      <th>11_y</th>\n",
       "      <th>12_y</th>\n",
       "      <th>13_y</th>\n",
       "      <th>14_y</th>\n",
       "      <th>15_y</th>\n",
       "      <th>16_y</th>\n",
       "      <th>17_y</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1_x</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998039</td>\n",
       "      <td>0.997844</td>\n",
       "      <td>0.995700</td>\n",
       "      <td>0.994926</td>\n",
       "      <td>0.952709</td>\n",
       "      <td>0.956751</td>\n",
       "      <td>0.583042</td>\n",
       "      <td>0.588549</td>\n",
       "      <td>0.567609</td>\n",
       "      <td>...</td>\n",
       "      <td>0.224039</td>\n",
       "      <td>-0.104012</td>\n",
       "      <td>0.121761</td>\n",
       "      <td>-0.067104</td>\n",
       "      <td>0.104824</td>\n",
       "      <td>-0.076732</td>\n",
       "      <td>0.138374</td>\n",
       "      <td>-0.164113</td>\n",
       "      <td>0.215662</td>\n",
       "      <td>0.341673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_x</th>\n",
       "      <td>0.998039</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999667</td>\n",
       "      <td>0.997437</td>\n",
       "      <td>0.996860</td>\n",
       "      <td>0.943564</td>\n",
       "      <td>0.947252</td>\n",
       "      <td>0.572502</td>\n",
       "      <td>0.578259</td>\n",
       "      <td>0.575627</td>\n",
       "      <td>...</td>\n",
       "      <td>0.234923</td>\n",
       "      <td>-0.097371</td>\n",
       "      <td>0.129010</td>\n",
       "      <td>-0.079874</td>\n",
       "      <td>0.104086</td>\n",
       "      <td>-0.093967</td>\n",
       "      <td>0.155911</td>\n",
       "      <td>-0.176812</td>\n",
       "      <td>0.219017</td>\n",
       "      <td>0.300589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_x</th>\n",
       "      <td>0.997844</td>\n",
       "      <td>0.999667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996829</td>\n",
       "      <td>0.996853</td>\n",
       "      <td>0.941172</td>\n",
       "      <td>0.946170</td>\n",
       "      <td>0.569052</td>\n",
       "      <td>0.576690</td>\n",
       "      <td>0.576046</td>\n",
       "      <td>...</td>\n",
       "      <td>0.235410</td>\n",
       "      <td>-0.100278</td>\n",
       "      <td>0.129614</td>\n",
       "      <td>-0.084265</td>\n",
       "      <td>0.101632</td>\n",
       "      <td>-0.098475</td>\n",
       "      <td>0.159378</td>\n",
       "      <td>-0.182077</td>\n",
       "      <td>0.220013</td>\n",
       "      <td>0.298172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4_x</th>\n",
       "      <td>0.995700</td>\n",
       "      <td>0.997437</td>\n",
       "      <td>0.996829</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997696</td>\n",
       "      <td>0.954678</td>\n",
       "      <td>0.956737</td>\n",
       "      <td>0.602861</td>\n",
       "      <td>0.604431</td>\n",
       "      <td>0.590388</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228198</td>\n",
       "      <td>-0.105833</td>\n",
       "      <td>0.119764</td>\n",
       "      <td>-0.064791</td>\n",
       "      <td>0.113489</td>\n",
       "      <td>-0.085685</td>\n",
       "      <td>0.141903</td>\n",
       "      <td>-0.166237</td>\n",
       "      <td>0.222908</td>\n",
       "      <td>0.310731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5_x</th>\n",
       "      <td>0.994926</td>\n",
       "      <td>0.996860</td>\n",
       "      <td>0.996853</td>\n",
       "      <td>0.997696</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.951815</td>\n",
       "      <td>0.957361</td>\n",
       "      <td>0.592259</td>\n",
       "      <td>0.601820</td>\n",
       "      <td>0.584815</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232072</td>\n",
       "      <td>-0.088387</td>\n",
       "      <td>0.136466</td>\n",
       "      <td>-0.095793</td>\n",
       "      <td>0.084356</td>\n",
       "      <td>-0.083430</td>\n",
       "      <td>0.145055</td>\n",
       "      <td>-0.188734</td>\n",
       "      <td>0.201952</td>\n",
       "      <td>0.309453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6_x</th>\n",
       "      <td>0.952709</td>\n",
       "      <td>0.943564</td>\n",
       "      <td>0.941172</td>\n",
       "      <td>0.954678</td>\n",
       "      <td>0.951815</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.983856</td>\n",
       "      <td>0.751357</td>\n",
       "      <td>0.742581</td>\n",
       "      <td>0.634628</td>\n",
       "      <td>...</td>\n",
       "      <td>0.173009</td>\n",
       "      <td>-0.064274</td>\n",
       "      <td>0.116608</td>\n",
       "      <td>-0.036414</td>\n",
       "      <td>0.060884</td>\n",
       "      <td>0.060999</td>\n",
       "      <td>0.015900</td>\n",
       "      <td>-0.104399</td>\n",
       "      <td>0.133075</td>\n",
       "      <td>0.466240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7_x</th>\n",
       "      <td>0.956751</td>\n",
       "      <td>0.947252</td>\n",
       "      <td>0.946170</td>\n",
       "      <td>0.956737</td>\n",
       "      <td>0.957361</td>\n",
       "      <td>0.983856</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.732863</td>\n",
       "      <td>0.748849</td>\n",
       "      <td>0.621902</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156591</td>\n",
       "      <td>-0.090410</td>\n",
       "      <td>0.095773</td>\n",
       "      <td>-0.036250</td>\n",
       "      <td>0.064012</td>\n",
       "      <td>0.038773</td>\n",
       "      <td>0.003441</td>\n",
       "      <td>-0.104989</td>\n",
       "      <td>0.141819</td>\n",
       "      <td>0.472527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8_x</th>\n",
       "      <td>0.583042</td>\n",
       "      <td>0.572502</td>\n",
       "      <td>0.569052</td>\n",
       "      <td>0.602861</td>\n",
       "      <td>0.592259</td>\n",
       "      <td>0.751357</td>\n",
       "      <td>0.732863</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.975522</td>\n",
       "      <td>0.735120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031899</td>\n",
       "      <td>-0.037885</td>\n",
       "      <td>0.044048</td>\n",
       "      <td>0.087686</td>\n",
       "      <td>0.048325</td>\n",
       "      <td>0.276466</td>\n",
       "      <td>-0.248980</td>\n",
       "      <td>0.104131</td>\n",
       "      <td>-0.010359</td>\n",
       "      <td>0.397643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9_x</th>\n",
       "      <td>0.588549</td>\n",
       "      <td>0.578259</td>\n",
       "      <td>0.576690</td>\n",
       "      <td>0.604431</td>\n",
       "      <td>0.601820</td>\n",
       "      <td>0.742581</td>\n",
       "      <td>0.748849</td>\n",
       "      <td>0.975522</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.722057</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024481</td>\n",
       "      <td>-0.033608</td>\n",
       "      <td>0.070537</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.026877</td>\n",
       "      <td>0.264549</td>\n",
       "      <td>-0.233267</td>\n",
       "      <td>0.039490</td>\n",
       "      <td>-0.044752</td>\n",
       "      <td>0.403667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10_x</th>\n",
       "      <td>0.567609</td>\n",
       "      <td>0.575627</td>\n",
       "      <td>0.576046</td>\n",
       "      <td>0.590388</td>\n",
       "      <td>0.584815</td>\n",
       "      <td>0.634628</td>\n",
       "      <td>0.621902</td>\n",
       "      <td>0.735120</td>\n",
       "      <td>0.722057</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.147299</td>\n",
       "      <td>0.010622</td>\n",
       "      <td>0.037156</td>\n",
       "      <td>-0.011528</td>\n",
       "      <td>0.027760</td>\n",
       "      <td>0.087406</td>\n",
       "      <td>-0.033764</td>\n",
       "      <td>-0.032226</td>\n",
       "      <td>0.074473</td>\n",
       "      <td>0.105376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11_x</th>\n",
       "      <td>0.565680</td>\n",
       "      <td>0.571988</td>\n",
       "      <td>0.572778</td>\n",
       "      <td>0.585913</td>\n",
       "      <td>0.581263</td>\n",
       "      <td>0.629292</td>\n",
       "      <td>0.626186</td>\n",
       "      <td>0.733013</td>\n",
       "      <td>0.731385</td>\n",
       "      <td>0.988333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104965</td>\n",
       "      <td>0.003374</td>\n",
       "      <td>0.001604</td>\n",
       "      <td>0.008075</td>\n",
       "      <td>0.035826</td>\n",
       "      <td>0.087604</td>\n",
       "      <td>-0.062874</td>\n",
       "      <td>-0.018180</td>\n",
       "      <td>0.066193</td>\n",
       "      <td>0.111526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12_x</th>\n",
       "      <td>0.592896</td>\n",
       "      <td>0.563546</td>\n",
       "      <td>0.560138</td>\n",
       "      <td>0.586273</td>\n",
       "      <td>0.585481</td>\n",
       "      <td>0.739177</td>\n",
       "      <td>0.731525</td>\n",
       "      <td>0.660718</td>\n",
       "      <td>0.668065</td>\n",
       "      <td>0.346804</td>\n",
       "      <td>...</td>\n",
       "      <td>0.116329</td>\n",
       "      <td>-0.123424</td>\n",
       "      <td>0.121921</td>\n",
       "      <td>-0.004365</td>\n",
       "      <td>0.075731</td>\n",
       "      <td>0.165352</td>\n",
       "      <td>-0.110586</td>\n",
       "      <td>-0.051576</td>\n",
       "      <td>0.105997</td>\n",
       "      <td>0.696326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13_x</th>\n",
       "      <td>0.589419</td>\n",
       "      <td>0.559769</td>\n",
       "      <td>0.557102</td>\n",
       "      <td>0.581537</td>\n",
       "      <td>0.584472</td>\n",
       "      <td>0.730986</td>\n",
       "      <td>0.736356</td>\n",
       "      <td>0.655980</td>\n",
       "      <td>0.679000</td>\n",
       "      <td>0.349397</td>\n",
       "      <td>...</td>\n",
       "      <td>0.114082</td>\n",
       "      <td>-0.102836</td>\n",
       "      <td>0.137604</td>\n",
       "      <td>-0.044241</td>\n",
       "      <td>0.032529</td>\n",
       "      <td>0.167728</td>\n",
       "      <td>-0.115771</td>\n",
       "      <td>-0.072006</td>\n",
       "      <td>0.079358</td>\n",
       "      <td>0.697033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14_x</th>\n",
       "      <td>-0.441769</td>\n",
       "      <td>-0.429084</td>\n",
       "      <td>-0.429517</td>\n",
       "      <td>-0.423544</td>\n",
       "      <td>-0.425981</td>\n",
       "      <td>-0.453444</td>\n",
       "      <td>-0.459870</td>\n",
       "      <td>-0.349964</td>\n",
       "      <td>-0.379003</td>\n",
       "      <td>-0.293631</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.207105</td>\n",
       "      <td>-0.019305</td>\n",
       "      <td>-0.181313</td>\n",
       "      <td>0.077215</td>\n",
       "      <td>0.087606</td>\n",
       "      <td>-0.080883</td>\n",
       "      <td>0.010108</td>\n",
       "      <td>0.087392</td>\n",
       "      <td>0.017283</td>\n",
       "      <td>-0.602592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15_x</th>\n",
       "      <td>-0.431585</td>\n",
       "      <td>-0.416593</td>\n",
       "      <td>-0.417294</td>\n",
       "      <td>-0.414643</td>\n",
       "      <td>-0.407054</td>\n",
       "      <td>-0.440863</td>\n",
       "      <td>-0.446945</td>\n",
       "      <td>-0.375400</td>\n",
       "      <td>-0.372154</td>\n",
       "      <td>-0.292173</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.128264</td>\n",
       "      <td>0.128277</td>\n",
       "      <td>-0.020475</td>\n",
       "      <td>-0.111324</td>\n",
       "      <td>-0.094345</td>\n",
       "      <td>-0.100148</td>\n",
       "      <td>0.031371</td>\n",
       "      <td>-0.068749</td>\n",
       "      <td>-0.106134</td>\n",
       "      <td>-0.591942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16_x</th>\n",
       "      <td>-0.632029</td>\n",
       "      <td>-0.647903</td>\n",
       "      <td>-0.649369</td>\n",
       "      <td>-0.636863</td>\n",
       "      <td>-0.646078</td>\n",
       "      <td>-0.555656</td>\n",
       "      <td>-0.564256</td>\n",
       "      <td>-0.311042</td>\n",
       "      <td>-0.341626</td>\n",
       "      <td>-0.463958</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.291666</td>\n",
       "      <td>-0.063608</td>\n",
       "      <td>-0.225808</td>\n",
       "      <td>0.212051</td>\n",
       "      <td>0.094605</td>\n",
       "      <td>0.131388</td>\n",
       "      <td>-0.197203</td>\n",
       "      <td>0.270099</td>\n",
       "      <td>-0.052354</td>\n",
       "      <td>-0.189415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17_x</th>\n",
       "      <td>-0.636251</td>\n",
       "      <td>-0.647424</td>\n",
       "      <td>-0.648953</td>\n",
       "      <td>-0.646465</td>\n",
       "      <td>-0.633328</td>\n",
       "      <td>-0.553761</td>\n",
       "      <td>-0.560282</td>\n",
       "      <td>-0.342377</td>\n",
       "      <td>-0.318922</td>\n",
       "      <td>-0.461896</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.182610</td>\n",
       "      <td>0.203085</td>\n",
       "      <td>0.048680</td>\n",
       "      <td>-0.116758</td>\n",
       "      <td>-0.235379</td>\n",
       "      <td>0.153117</td>\n",
       "      <td>-0.189531</td>\n",
       "      <td>0.014515</td>\n",
       "      <td>-0.318426</td>\n",
       "      <td>-0.182338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_y</th>\n",
       "      <td>0.004096</td>\n",
       "      <td>0.009630</td>\n",
       "      <td>0.006045</td>\n",
       "      <td>-0.001397</td>\n",
       "      <td>0.018088</td>\n",
       "      <td>0.048395</td>\n",
       "      <td>-0.024182</td>\n",
       "      <td>-0.005140</td>\n",
       "      <td>-0.002394</td>\n",
       "      <td>0.031391</td>\n",
       "      <td>...</td>\n",
       "      <td>0.308097</td>\n",
       "      <td>0.377565</td>\n",
       "      <td>0.370423</td>\n",
       "      <td>-0.194667</td>\n",
       "      <td>-0.194687</td>\n",
       "      <td>0.399441</td>\n",
       "      <td>0.356519</td>\n",
       "      <td>-0.016000</td>\n",
       "      <td>-0.047790</td>\n",
       "      <td>-0.001403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_y</th>\n",
       "      <td>-0.038061</td>\n",
       "      <td>-0.035360</td>\n",
       "      <td>-0.039465</td>\n",
       "      <td>-0.042510</td>\n",
       "      <td>-0.027787</td>\n",
       "      <td>0.019871</td>\n",
       "      <td>-0.051154</td>\n",
       "      <td>0.006041</td>\n",
       "      <td>-0.000553</td>\n",
       "      <td>0.018775</td>\n",
       "      <td>...</td>\n",
       "      <td>0.207074</td>\n",
       "      <td>0.350241</td>\n",
       "      <td>0.263931</td>\n",
       "      <td>-0.087185</td>\n",
       "      <td>-0.141095</td>\n",
       "      <td>0.504292</td>\n",
       "      <td>0.285136</td>\n",
       "      <td>0.123655</td>\n",
       "      <td>-0.048275</td>\n",
       "      <td>0.001488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_y</th>\n",
       "      <td>0.041950</td>\n",
       "      <td>0.047612</td>\n",
       "      <td>0.044681</td>\n",
       "      <td>0.036540</td>\n",
       "      <td>0.051565</td>\n",
       "      <td>0.065654</td>\n",
       "      <td>-0.002977</td>\n",
       "      <td>-0.017273</td>\n",
       "      <td>-0.016235</td>\n",
       "      <td>0.029564</td>\n",
       "      <td>...</td>\n",
       "      <td>0.369515</td>\n",
       "      <td>0.263303</td>\n",
       "      <td>0.347406</td>\n",
       "      <td>-0.147198</td>\n",
       "      <td>-0.080523</td>\n",
       "      <td>0.325745</td>\n",
       "      <td>0.471984</td>\n",
       "      <td>-0.009886</td>\n",
       "      <td>0.105435</td>\n",
       "      <td>0.001987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4_y</th>\n",
       "      <td>-0.083785</td>\n",
       "      <td>-0.088091</td>\n",
       "      <td>-0.092618</td>\n",
       "      <td>-0.082418</td>\n",
       "      <td>-0.091614</td>\n",
       "      <td>-0.021421</td>\n",
       "      <td>-0.074711</td>\n",
       "      <td>0.057678</td>\n",
       "      <td>0.006169</td>\n",
       "      <td>0.022306</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081044</td>\n",
       "      <td>0.052027</td>\n",
       "      <td>-0.170472</td>\n",
       "      <td>0.413520</td>\n",
       "      <td>0.269228</td>\n",
       "      <td>0.620484</td>\n",
       "      <td>0.115376</td>\n",
       "      <td>0.587099</td>\n",
       "      <td>0.179473</td>\n",
       "      <td>-0.008896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5_y</th>\n",
       "      <td>0.097904</td>\n",
       "      <td>0.099563</td>\n",
       "      <td>0.098159</td>\n",
       "      <td>0.096881</td>\n",
       "      <td>0.087781</td>\n",
       "      <td>0.081160</td>\n",
       "      <td>0.034719</td>\n",
       "      <td>-0.002960</td>\n",
       "      <td>-0.034674</td>\n",
       "      <td>0.032634</td>\n",
       "      <td>...</td>\n",
       "      <td>0.342031</td>\n",
       "      <td>-0.186473</td>\n",
       "      <td>0.052771</td>\n",
       "      <td>0.265539</td>\n",
       "      <td>0.440107</td>\n",
       "      <td>0.154313</td>\n",
       "      <td>0.587545</td>\n",
       "      <td>0.239917</td>\n",
       "      <td>0.582226</td>\n",
       "      <td>0.018799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6_y</th>\n",
       "      <td>-0.208310</td>\n",
       "      <td>-0.219507</td>\n",
       "      <td>-0.226194</td>\n",
       "      <td>-0.202636</td>\n",
       "      <td>-0.222259</td>\n",
       "      <td>-0.100392</td>\n",
       "      <td>-0.142274</td>\n",
       "      <td>0.112494</td>\n",
       "      <td>0.030076</td>\n",
       "      <td>-0.009669</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.322063</td>\n",
       "      <td>0.095570</td>\n",
       "      <td>-0.384426</td>\n",
       "      <td>0.669838</td>\n",
       "      <td>0.335984</td>\n",
       "      <td>0.709643</td>\n",
       "      <td>-0.261953</td>\n",
       "      <td>0.830128</td>\n",
       "      <td>0.039992</td>\n",
       "      <td>0.006609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7_y</th>\n",
       "      <td>0.239486</td>\n",
       "      <td>0.245339</td>\n",
       "      <td>0.244947</td>\n",
       "      <td>0.239620</td>\n",
       "      <td>0.222154</td>\n",
       "      <td>0.156880</td>\n",
       "      <td>0.129136</td>\n",
       "      <td>-0.024359</td>\n",
       "      <td>-0.067782</td>\n",
       "      <td>0.054557</td>\n",
       "      <td>...</td>\n",
       "      <td>0.541702</td>\n",
       "      <td>-0.371119</td>\n",
       "      <td>0.112466</td>\n",
       "      <td>0.324312</td>\n",
       "      <td>0.673797</td>\n",
       "      <td>-0.233046</td>\n",
       "      <td>0.714950</td>\n",
       "      <td>0.088145</td>\n",
       "      <td>0.848158</td>\n",
       "      <td>0.010294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8_y</th>\n",
       "      <td>-0.197440</td>\n",
       "      <td>-0.199029</td>\n",
       "      <td>-0.203705</td>\n",
       "      <td>-0.198965</td>\n",
       "      <td>-0.193001</td>\n",
       "      <td>-0.117342</td>\n",
       "      <td>-0.142914</td>\n",
       "      <td>0.022007</td>\n",
       "      <td>-0.007473</td>\n",
       "      <td>-0.092103</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042015</td>\n",
       "      <td>0.810877</td>\n",
       "      <td>0.239810</td>\n",
       "      <td>-0.028256</td>\n",
       "      <td>-0.365860</td>\n",
       "      <td>0.669449</td>\n",
       "      <td>-0.332775</td>\n",
       "      <td>0.311317</td>\n",
       "      <td>-0.520879</td>\n",
       "      <td>-0.048657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9_y</th>\n",
       "      <td>0.224039</td>\n",
       "      <td>0.234923</td>\n",
       "      <td>0.235410</td>\n",
       "      <td>0.228198</td>\n",
       "      <td>0.232072</td>\n",
       "      <td>0.173009</td>\n",
       "      <td>0.156591</td>\n",
       "      <td>0.031899</td>\n",
       "      <td>0.024481</td>\n",
       "      <td>0.147299</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.230420</td>\n",
       "      <td>0.808456</td>\n",
       "      <td>-0.339347</td>\n",
       "      <td>-0.004345</td>\n",
       "      <td>-0.310763</td>\n",
       "      <td>0.677365</td>\n",
       "      <td>-0.482842</td>\n",
       "      <td>0.361664</td>\n",
       "      <td>0.070515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10_y</th>\n",
       "      <td>-0.104012</td>\n",
       "      <td>-0.097371</td>\n",
       "      <td>-0.100278</td>\n",
       "      <td>-0.105833</td>\n",
       "      <td>-0.088387</td>\n",
       "      <td>-0.064274</td>\n",
       "      <td>-0.090410</td>\n",
       "      <td>-0.037885</td>\n",
       "      <td>-0.033608</td>\n",
       "      <td>0.010622</td>\n",
       "      <td>...</td>\n",
       "      <td>0.230420</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.557468</td>\n",
       "      <td>-0.414418</td>\n",
       "      <td>-0.615452</td>\n",
       "      <td>0.360414</td>\n",
       "      <td>-0.109192</td>\n",
       "      <td>-0.151245</td>\n",
       "      <td>-0.564731</td>\n",
       "      <td>-0.074850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11_y</th>\n",
       "      <td>0.121761</td>\n",
       "      <td>0.129010</td>\n",
       "      <td>0.129614</td>\n",
       "      <td>0.119764</td>\n",
       "      <td>0.136466</td>\n",
       "      <td>0.116608</td>\n",
       "      <td>0.095773</td>\n",
       "      <td>0.044048</td>\n",
       "      <td>0.070537</td>\n",
       "      <td>0.037156</td>\n",
       "      <td>...</td>\n",
       "      <td>0.808456</td>\n",
       "      <td>0.557468</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.620337</td>\n",
       "      <td>-0.428816</td>\n",
       "      <td>-0.110760</td>\n",
       "      <td>0.385957</td>\n",
       "      <td>-0.584657</td>\n",
       "      <td>-0.110985</td>\n",
       "      <td>0.094309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12_y</th>\n",
       "      <td>-0.067104</td>\n",
       "      <td>-0.079874</td>\n",
       "      <td>-0.084265</td>\n",
       "      <td>-0.064791</td>\n",
       "      <td>-0.095793</td>\n",
       "      <td>-0.036414</td>\n",
       "      <td>-0.036250</td>\n",
       "      <td>0.087686</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.011528</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.339347</td>\n",
       "      <td>-0.414418</td>\n",
       "      <td>-0.620337</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.838302</td>\n",
       "      <td>0.176672</td>\n",
       "      <td>-0.217986</td>\n",
       "      <td>0.813845</td>\n",
       "      <td>0.474736</td>\n",
       "      <td>0.053299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13_y</th>\n",
       "      <td>0.104824</td>\n",
       "      <td>0.104086</td>\n",
       "      <td>0.101632</td>\n",
       "      <td>0.113489</td>\n",
       "      <td>0.084356</td>\n",
       "      <td>0.060884</td>\n",
       "      <td>0.064012</td>\n",
       "      <td>0.048325</td>\n",
       "      <td>-0.026877</td>\n",
       "      <td>0.027760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004345</td>\n",
       "      <td>-0.615452</td>\n",
       "      <td>-0.428816</td>\n",
       "      <td>0.838302</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.200480</td>\n",
       "      <td>0.181325</td>\n",
       "      <td>0.504195</td>\n",
       "      <td>0.812456</td>\n",
       "      <td>-0.027717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14_y</th>\n",
       "      <td>-0.076732</td>\n",
       "      <td>-0.093967</td>\n",
       "      <td>-0.098475</td>\n",
       "      <td>-0.085685</td>\n",
       "      <td>-0.083430</td>\n",
       "      <td>0.060999</td>\n",
       "      <td>0.038773</td>\n",
       "      <td>0.276466</td>\n",
       "      <td>0.264549</td>\n",
       "      <td>0.087406</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.310763</td>\n",
       "      <td>0.360414</td>\n",
       "      <td>-0.110760</td>\n",
       "      <td>0.176672</td>\n",
       "      <td>-0.200480</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.458620</td>\n",
       "      <td>0.616636</td>\n",
       "      <td>-0.409062</td>\n",
       "      <td>0.204168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15_y</th>\n",
       "      <td>0.138374</td>\n",
       "      <td>0.155911</td>\n",
       "      <td>0.159378</td>\n",
       "      <td>0.141903</td>\n",
       "      <td>0.145055</td>\n",
       "      <td>0.015900</td>\n",
       "      <td>0.003441</td>\n",
       "      <td>-0.248980</td>\n",
       "      <td>-0.233267</td>\n",
       "      <td>-0.033764</td>\n",
       "      <td>...</td>\n",
       "      <td>0.677365</td>\n",
       "      <td>-0.109192</td>\n",
       "      <td>0.385957</td>\n",
       "      <td>-0.217986</td>\n",
       "      <td>0.181325</td>\n",
       "      <td>-0.458620</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.414223</td>\n",
       "      <td>0.643928</td>\n",
       "      <td>-0.147063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16_y</th>\n",
       "      <td>-0.164113</td>\n",
       "      <td>-0.176812</td>\n",
       "      <td>-0.182077</td>\n",
       "      <td>-0.166237</td>\n",
       "      <td>-0.188734</td>\n",
       "      <td>-0.104399</td>\n",
       "      <td>-0.104989</td>\n",
       "      <td>0.104131</td>\n",
       "      <td>0.039490</td>\n",
       "      <td>-0.032226</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.482842</td>\n",
       "      <td>-0.151245</td>\n",
       "      <td>-0.584657</td>\n",
       "      <td>0.813845</td>\n",
       "      <td>0.504195</td>\n",
       "      <td>0.616636</td>\n",
       "      <td>-0.414223</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.102966</td>\n",
       "      <td>0.028712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17_y</th>\n",
       "      <td>0.215662</td>\n",
       "      <td>0.219017</td>\n",
       "      <td>0.220013</td>\n",
       "      <td>0.222908</td>\n",
       "      <td>0.201952</td>\n",
       "      <td>0.133075</td>\n",
       "      <td>0.141819</td>\n",
       "      <td>-0.010359</td>\n",
       "      <td>-0.044752</td>\n",
       "      <td>0.074473</td>\n",
       "      <td>...</td>\n",
       "      <td>0.361664</td>\n",
       "      <td>-0.564731</td>\n",
       "      <td>-0.110985</td>\n",
       "      <td>0.474736</td>\n",
       "      <td>0.812456</td>\n",
       "      <td>-0.409062</td>\n",
       "      <td>0.643928</td>\n",
       "      <td>0.102966</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.002311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y</th>\n",
       "      <td>0.341673</td>\n",
       "      <td>0.300589</td>\n",
       "      <td>0.298172</td>\n",
       "      <td>0.310731</td>\n",
       "      <td>0.309453</td>\n",
       "      <td>0.466240</td>\n",
       "      <td>0.472527</td>\n",
       "      <td>0.397643</td>\n",
       "      <td>0.403667</td>\n",
       "      <td>0.105376</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070515</td>\n",
       "      <td>-0.074850</td>\n",
       "      <td>0.094309</td>\n",
       "      <td>0.053299</td>\n",
       "      <td>-0.027717</td>\n",
       "      <td>0.204168</td>\n",
       "      <td>-0.147063</td>\n",
       "      <td>0.028712</td>\n",
       "      <td>-0.002311</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           1_x       2_x       3_x       4_x       5_x       6_x       7_x  \\\n",
       "1_x   1.000000  0.998039  0.997844  0.995700  0.994926  0.952709  0.956751   \n",
       "2_x   0.998039  1.000000  0.999667  0.997437  0.996860  0.943564  0.947252   \n",
       "3_x   0.997844  0.999667  1.000000  0.996829  0.996853  0.941172  0.946170   \n",
       "4_x   0.995700  0.997437  0.996829  1.000000  0.997696  0.954678  0.956737   \n",
       "5_x   0.994926  0.996860  0.996853  0.997696  1.000000  0.951815  0.957361   \n",
       "6_x   0.952709  0.943564  0.941172  0.954678  0.951815  1.000000  0.983856   \n",
       "7_x   0.956751  0.947252  0.946170  0.956737  0.957361  0.983856  1.000000   \n",
       "8_x   0.583042  0.572502  0.569052  0.602861  0.592259  0.751357  0.732863   \n",
       "9_x   0.588549  0.578259  0.576690  0.604431  0.601820  0.742581  0.748849   \n",
       "10_x  0.567609  0.575627  0.576046  0.590388  0.584815  0.634628  0.621902   \n",
       "11_x  0.565680  0.571988  0.572778  0.585913  0.581263  0.629292  0.626186   \n",
       "12_x  0.592896  0.563546  0.560138  0.586273  0.585481  0.739177  0.731525   \n",
       "13_x  0.589419  0.559769  0.557102  0.581537  0.584472  0.730986  0.736356   \n",
       "14_x -0.441769 -0.429084 -0.429517 -0.423544 -0.425981 -0.453444 -0.459870   \n",
       "15_x -0.431585 -0.416593 -0.417294 -0.414643 -0.407054 -0.440863 -0.446945   \n",
       "16_x -0.632029 -0.647903 -0.649369 -0.636863 -0.646078 -0.555656 -0.564256   \n",
       "17_x -0.636251 -0.647424 -0.648953 -0.646465 -0.633328 -0.553761 -0.560282   \n",
       "1_y   0.004096  0.009630  0.006045 -0.001397  0.018088  0.048395 -0.024182   \n",
       "2_y  -0.038061 -0.035360 -0.039465 -0.042510 -0.027787  0.019871 -0.051154   \n",
       "3_y   0.041950  0.047612  0.044681  0.036540  0.051565  0.065654 -0.002977   \n",
       "4_y  -0.083785 -0.088091 -0.092618 -0.082418 -0.091614 -0.021421 -0.074711   \n",
       "5_y   0.097904  0.099563  0.098159  0.096881  0.087781  0.081160  0.034719   \n",
       "6_y  -0.208310 -0.219507 -0.226194 -0.202636 -0.222259 -0.100392 -0.142274   \n",
       "7_y   0.239486  0.245339  0.244947  0.239620  0.222154  0.156880  0.129136   \n",
       "8_y  -0.197440 -0.199029 -0.203705 -0.198965 -0.193001 -0.117342 -0.142914   \n",
       "9_y   0.224039  0.234923  0.235410  0.228198  0.232072  0.173009  0.156591   \n",
       "10_y -0.104012 -0.097371 -0.100278 -0.105833 -0.088387 -0.064274 -0.090410   \n",
       "11_y  0.121761  0.129010  0.129614  0.119764  0.136466  0.116608  0.095773   \n",
       "12_y -0.067104 -0.079874 -0.084265 -0.064791 -0.095793 -0.036414 -0.036250   \n",
       "13_y  0.104824  0.104086  0.101632  0.113489  0.084356  0.060884  0.064012   \n",
       "14_y -0.076732 -0.093967 -0.098475 -0.085685 -0.083430  0.060999  0.038773   \n",
       "15_y  0.138374  0.155911  0.159378  0.141903  0.145055  0.015900  0.003441   \n",
       "16_y -0.164113 -0.176812 -0.182077 -0.166237 -0.188734 -0.104399 -0.104989   \n",
       "17_y  0.215662  0.219017  0.220013  0.222908  0.201952  0.133075  0.141819   \n",
       "Y     0.341673  0.300589  0.298172  0.310731  0.309453  0.466240  0.472527   \n",
       "\n",
       "           8_x       9_x      10_x  ...       9_y      10_y      11_y  \\\n",
       "1_x   0.583042  0.588549  0.567609  ...  0.224039 -0.104012  0.121761   \n",
       "2_x   0.572502  0.578259  0.575627  ...  0.234923 -0.097371  0.129010   \n",
       "3_x   0.569052  0.576690  0.576046  ...  0.235410 -0.100278  0.129614   \n",
       "4_x   0.602861  0.604431  0.590388  ...  0.228198 -0.105833  0.119764   \n",
       "5_x   0.592259  0.601820  0.584815  ...  0.232072 -0.088387  0.136466   \n",
       "6_x   0.751357  0.742581  0.634628  ...  0.173009 -0.064274  0.116608   \n",
       "7_x   0.732863  0.748849  0.621902  ...  0.156591 -0.090410  0.095773   \n",
       "8_x   1.000000  0.975522  0.735120  ...  0.031899 -0.037885  0.044048   \n",
       "9_x   0.975522  1.000000  0.722057  ...  0.024481 -0.033608  0.070537   \n",
       "10_x  0.735120  0.722057  1.000000  ...  0.147299  0.010622  0.037156   \n",
       "11_x  0.733013  0.731385  0.988333  ...  0.104965  0.003374  0.001604   \n",
       "12_x  0.660718  0.668065  0.346804  ...  0.116329 -0.123424  0.121921   \n",
       "13_x  0.655980  0.679000  0.349397  ...  0.114082 -0.102836  0.137604   \n",
       "14_x -0.349964 -0.379003 -0.293631  ... -0.207105 -0.019305 -0.181313   \n",
       "15_x -0.375400 -0.372154 -0.292173  ... -0.128264  0.128277 -0.020475   \n",
       "16_x -0.311042 -0.341626 -0.463958  ... -0.291666 -0.063608 -0.225808   \n",
       "17_x -0.342377 -0.318922 -0.461896  ... -0.182610  0.203085  0.048680   \n",
       "1_y  -0.005140 -0.002394  0.031391  ...  0.308097  0.377565  0.370423   \n",
       "2_y   0.006041 -0.000553  0.018775  ...  0.207074  0.350241  0.263931   \n",
       "3_y  -0.017273 -0.016235  0.029564  ...  0.369515  0.263303  0.347406   \n",
       "4_y   0.057678  0.006169  0.022306  ... -0.081044  0.052027 -0.170472   \n",
       "5_y  -0.002960 -0.034674  0.032634  ...  0.342031 -0.186473  0.052771   \n",
       "6_y   0.112494  0.030076 -0.009669  ... -0.322063  0.095570 -0.384426   \n",
       "7_y  -0.024359 -0.067782  0.054557  ...  0.541702 -0.371119  0.112466   \n",
       "8_y   0.022007 -0.007473 -0.092103  ... -0.042015  0.810877  0.239810   \n",
       "9_y   0.031899  0.024481  0.147299  ...  1.000000  0.230420  0.808456   \n",
       "10_y -0.037885 -0.033608  0.010622  ...  0.230420  1.000000  0.557468   \n",
       "11_y  0.044048  0.070537  0.037156  ...  0.808456  0.557468  1.000000   \n",
       "12_y  0.087686 -0.002592 -0.011528  ... -0.339347 -0.414418 -0.620337   \n",
       "13_y  0.048325 -0.026877  0.027760  ... -0.004345 -0.615452 -0.428816   \n",
       "14_y  0.276466  0.264549  0.087406  ... -0.310763  0.360414 -0.110760   \n",
       "15_y -0.248980 -0.233267 -0.033764  ...  0.677365 -0.109192  0.385957   \n",
       "16_y  0.104131  0.039490 -0.032226  ... -0.482842 -0.151245 -0.584657   \n",
       "17_y -0.010359 -0.044752  0.074473  ...  0.361664 -0.564731 -0.110985   \n",
       "Y     0.397643  0.403667  0.105376  ...  0.070515 -0.074850  0.094309   \n",
       "\n",
       "          12_y      13_y      14_y      15_y      16_y      17_y         Y  \n",
       "1_x  -0.067104  0.104824 -0.076732  0.138374 -0.164113  0.215662  0.341673  \n",
       "2_x  -0.079874  0.104086 -0.093967  0.155911 -0.176812  0.219017  0.300589  \n",
       "3_x  -0.084265  0.101632 -0.098475  0.159378 -0.182077  0.220013  0.298172  \n",
       "4_x  -0.064791  0.113489 -0.085685  0.141903 -0.166237  0.222908  0.310731  \n",
       "5_x  -0.095793  0.084356 -0.083430  0.145055 -0.188734  0.201952  0.309453  \n",
       "6_x  -0.036414  0.060884  0.060999  0.015900 -0.104399  0.133075  0.466240  \n",
       "7_x  -0.036250  0.064012  0.038773  0.003441 -0.104989  0.141819  0.472527  \n",
       "8_x   0.087686  0.048325  0.276466 -0.248980  0.104131 -0.010359  0.397643  \n",
       "9_x  -0.002592 -0.026877  0.264549 -0.233267  0.039490 -0.044752  0.403667  \n",
       "10_x -0.011528  0.027760  0.087406 -0.033764 -0.032226  0.074473  0.105376  \n",
       "11_x  0.008075  0.035826  0.087604 -0.062874 -0.018180  0.066193  0.111526  \n",
       "12_x -0.004365  0.075731  0.165352 -0.110586 -0.051576  0.105997  0.696326  \n",
       "13_x -0.044241  0.032529  0.167728 -0.115771 -0.072006  0.079358  0.697033  \n",
       "14_x  0.077215  0.087606 -0.080883  0.010108  0.087392  0.017283 -0.602592  \n",
       "15_x -0.111324 -0.094345 -0.100148  0.031371 -0.068749 -0.106134 -0.591942  \n",
       "16_x  0.212051  0.094605  0.131388 -0.197203  0.270099 -0.052354 -0.189415  \n",
       "17_x -0.116758 -0.235379  0.153117 -0.189531  0.014515 -0.318426 -0.182338  \n",
       "1_y  -0.194667 -0.194687  0.399441  0.356519 -0.016000 -0.047790 -0.001403  \n",
       "2_y  -0.087185 -0.141095  0.504292  0.285136  0.123655 -0.048275  0.001488  \n",
       "3_y  -0.147198 -0.080523  0.325745  0.471984 -0.009886  0.105435  0.001987  \n",
       "4_y   0.413520  0.269228  0.620484  0.115376  0.587099  0.179473 -0.008896  \n",
       "5_y   0.265539  0.440107  0.154313  0.587545  0.239917  0.582226  0.018799  \n",
       "6_y   0.669838  0.335984  0.709643 -0.261953  0.830128  0.039992  0.006609  \n",
       "7_y   0.324312  0.673797 -0.233046  0.714950  0.088145  0.848158  0.010294  \n",
       "8_y  -0.028256 -0.365860  0.669449 -0.332775  0.311317 -0.520879 -0.048657  \n",
       "9_y  -0.339347 -0.004345 -0.310763  0.677365 -0.482842  0.361664  0.070515  \n",
       "10_y -0.414418 -0.615452  0.360414 -0.109192 -0.151245 -0.564731 -0.074850  \n",
       "11_y -0.620337 -0.428816 -0.110760  0.385957 -0.584657 -0.110985  0.094309  \n",
       "12_y  1.000000  0.838302  0.176672 -0.217986  0.813845  0.474736  0.053299  \n",
       "13_y  0.838302  1.000000 -0.200480  0.181325  0.504195  0.812456 -0.027717  \n",
       "14_y  0.176672 -0.200480  1.000000 -0.458620  0.616636 -0.409062  0.204168  \n",
       "15_y -0.217986  0.181325 -0.458620  1.000000 -0.414223  0.643928 -0.147063  \n",
       "16_y  0.813845  0.504195  0.616636 -0.414223  1.000000  0.102966  0.028712  \n",
       "17_y  0.474736  0.812456 -0.409062  0.643928  0.102966  1.000000 -0.002311  \n",
       "Y     0.053299 -0.027717  0.204168 -0.147063  0.028712 -0.002311  1.000000  \n",
       "\n",
       "[35 rows x 35 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols = [col for col in columns if col != \"Y\"]\n",
    "x_df = df[x_cols]\n",
    "y_df = df[['Y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2001l\\AppData\\Local\\Temp\\ipykernel_46272\\2561977078.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y_df['Y'] = y_df['Y'].astype('int')\n"
     ]
    }
   ],
   "source": [
    "y_df['Y'] = y_df['Y'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Y    int32\n",
       "dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\2001l\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - accuracy: 0.3353 - loss: 0.7194 - val_accuracy: 0.0000e+00 - val_loss: 0.8064\n",
      "Epoch 2/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6780 - loss: 0.6758 - val_accuracy: 0.0000e+00 - val_loss: 0.9338\n",
      "Epoch 3/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6663 - loss: 0.6530 - val_accuracy: 0.0000e+00 - val_loss: 1.0366\n",
      "Epoch 4/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6898 - loss: 0.6356 - val_accuracy: 0.0000e+00 - val_loss: 1.1214\n",
      "Epoch 5/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6898 - loss: 0.6287 - val_accuracy: 0.0000e+00 - val_loss: 1.1656\n",
      "Epoch 6/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.6546 - loss: 0.6570 - val_accuracy: 0.0000e+00 - val_loss: 1.1696\n",
      "Epoch 7/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.6546 - loss: 0.6521 - val_accuracy: 0.0000e+00 - val_loss: 1.1805\n",
      "Epoch 8/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7054 - loss: 0.6098 - val_accuracy: 0.0000e+00 - val_loss: 1.1950\n",
      "Epoch 9/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.6741 - loss: 0.6354 - val_accuracy: 0.0000e+00 - val_loss: 1.1732\n",
      "Epoch 10/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.6898 - loss: 0.6178 - val_accuracy: 0.0000e+00 - val_loss: 1.1597\n",
      "Epoch 11/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6898 - loss: 0.6141 - val_accuracy: 0.0000e+00 - val_loss: 1.1449\n",
      "Epoch 12/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.6859 - loss: 0.6139 - val_accuracy: 0.0000e+00 - val_loss: 1.1158\n",
      "Epoch 13/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6937 - loss: 0.6053 - val_accuracy: 0.0000e+00 - val_loss: 1.0918\n",
      "Epoch 14/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.6780 - loss: 0.6185 - val_accuracy: 0.0000e+00 - val_loss: 1.0635\n",
      "Epoch 15/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7054 - loss: 0.5954 - val_accuracy: 0.0000e+00 - val_loss: 1.0364\n",
      "Epoch 16/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.6898 - loss: 0.6049 - val_accuracy: 0.0000e+00 - val_loss: 1.0102\n",
      "Epoch 17/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7054 - loss: 0.5909 - val_accuracy: 0.0000e+00 - val_loss: 0.9976\n",
      "Epoch 18/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.6780 - loss: 0.6083 - val_accuracy: 0.0000e+00 - val_loss: 0.9962\n",
      "Epoch 19/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7132 - loss: 0.5860 - val_accuracy: 0.0000e+00 - val_loss: 1.0320\n",
      "Epoch 20/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6819 - loss: 0.5997 - val_accuracy: 0.0000e+00 - val_loss: 1.0426\n",
      "Epoch 21/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.6859 - loss: 0.5942 - val_accuracy: 0.0000e+00 - val_loss: 1.0468\n",
      "Epoch 22/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7015 - loss: 0.5782 - val_accuracy: 0.0000e+00 - val_loss: 1.0742\n",
      "Epoch 23/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.6663 - loss: 0.6006 - val_accuracy: 0.0000e+00 - val_loss: 1.0788\n",
      "Epoch 24/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7054 - loss: 0.5664 - val_accuracy: 0.0000e+00 - val_loss: 1.0833\n",
      "Epoch 25/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.6819 - loss: 0.5799 - val_accuracy: 0.0000e+00 - val_loss: 1.0329\n",
      "Epoch 26/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.6585 - loss: 0.5933 - val_accuracy: 0.0000e+00 - val_loss: 0.9965\n",
      "Epoch 27/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.6741 - loss: 0.5858 - val_accuracy: 0.0000e+00 - val_loss: 1.0263\n",
      "Epoch 28/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.6819 - loss: 0.5784 - val_accuracy: 0.0000e+00 - val_loss: 1.0545\n",
      "Epoch 29/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.7054 - loss: 0.5581 - val_accuracy: 0.0000e+00 - val_loss: 1.0555\n",
      "Epoch 30/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.7015 - loss: 0.5532 - val_accuracy: 0.0000e+00 - val_loss: 1.0493\n",
      "Epoch 31/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.7093 - loss: 0.5416 - val_accuracy: 0.0000e+00 - val_loss: 1.0250\n",
      "Epoch 32/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.6859 - loss: 0.5618 - val_accuracy: 0.0000e+00 - val_loss: 0.9783\n",
      "Epoch 33/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6990 - loss: 0.5562 - val_accuracy: 0.0000e+00 - val_loss: 0.9761\n",
      "Epoch 34/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.7107 - loss: 0.5546 - val_accuracy: 0.0000e+00 - val_loss: 0.9781\n",
      "Epoch 35/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.7251 - loss: 0.5480 - val_accuracy: 0.0500 - val_loss: 0.9560\n",
      "Epoch 36/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7473 - loss: 0.5411 - val_accuracy: 0.0500 - val_loss: 0.9478\n",
      "Epoch 37/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7161 - loss: 0.5491 - val_accuracy: 0.0500 - val_loss: 0.9736\n",
      "Epoch 38/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7317 - loss: 0.5414 - val_accuracy: 0.0500 - val_loss: 1.0129\n",
      "Epoch 39/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7251 - loss: 0.5407 - val_accuracy: 0.0500 - val_loss: 1.0561\n",
      "Epoch 40/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.7212 - loss: 0.5300 - val_accuracy: 0.0500 - val_loss: 1.0471\n",
      "Epoch 41/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7669 - loss: 0.5043 - val_accuracy: 0.0500 - val_loss: 1.0337\n",
      "Epoch 42/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.6926 - loss: 0.5595 - val_accuracy: 0.1000 - val_loss: 0.9605\n",
      "Epoch 43/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7395 - loss: 0.5180 - val_accuracy: 0.1000 - val_loss: 0.9773\n",
      "Epoch 44/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.7278 - loss: 0.5212 - val_accuracy: 0.1000 - val_loss: 0.9809\n",
      "Epoch 45/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.7512 - loss: 0.5132 - val_accuracy: 0.1000 - val_loss: 0.9714\n",
      "Epoch 46/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7395 - loss: 0.5165 - val_accuracy: 0.1000 - val_loss: 0.9481\n",
      "Epoch 47/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7512 - loss: 0.4838 - val_accuracy: 0.1500 - val_loss: 0.9245\n",
      "Epoch 48/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.7278 - loss: 0.5150 - val_accuracy: 0.1500 - val_loss: 0.9241\n",
      "Epoch 49/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7786 - loss: 0.4823 - val_accuracy: 0.1500 - val_loss: 0.9283\n",
      "Epoch 50/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7356 - loss: 0.4980 - val_accuracy: 0.2000 - val_loss: 0.8619\n",
      "Epoch 51/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7617 - loss: 0.4974 - val_accuracy: 0.2000 - val_loss: 0.8682\n",
      "Epoch 52/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.7356 - loss: 0.5001 - val_accuracy: 0.1500 - val_loss: 0.9347\n",
      "Epoch 53/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.7512 - loss: 0.4787 - val_accuracy: 0.1000 - val_loss: 0.9852\n",
      "Epoch 54/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.7317 - loss: 0.4875 - val_accuracy: 0.1500 - val_loss: 0.9526\n",
      "Epoch 55/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7486 - loss: 0.4729 - val_accuracy: 0.2500 - val_loss: 0.8372\n",
      "Epoch 56/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7761 - loss: 0.4662 - val_accuracy: 0.4000 - val_loss: 0.7768\n",
      "Epoch 57/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7827 - loss: 0.4779 - val_accuracy: 0.3000 - val_loss: 0.8044\n",
      "Epoch 58/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7671 - loss: 0.4891 - val_accuracy: 0.2000 - val_loss: 0.8701\n",
      "Epoch 59/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7656 - loss: 0.4685 - val_accuracy: 0.2000 - val_loss: 0.9432\n",
      "Epoch 60/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7500 - loss: 0.4697 - val_accuracy: 0.2000 - val_loss: 0.9099\n",
      "Epoch 61/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.7239 - loss: 0.4824 - val_accuracy: 0.2000 - val_loss: 0.8629\n",
      "Epoch 62/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.7669 - loss: 0.4462 - val_accuracy: 0.2000 - val_loss: 0.8771\n",
      "Epoch 63/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.7917 - loss: 0.4518 - val_accuracy: 0.2000 - val_loss: 0.8643\n",
      "Epoch 64/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7527 - loss: 0.4779 - val_accuracy: 0.2500 - val_loss: 0.8460\n",
      "Epoch 65/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7722 - loss: 0.4420 - val_accuracy: 0.2000 - val_loss: 0.8707\n",
      "Epoch 66/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7644 - loss: 0.4627 - val_accuracy: 0.3000 - val_loss: 0.8664\n",
      "Epoch 67/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.7605 - loss: 0.4579 - val_accuracy: 0.2500 - val_loss: 0.8760\n",
      "Epoch 68/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.7292 - loss: 0.4744 - val_accuracy: 0.2000 - val_loss: 0.9086\n",
      "Epoch 69/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.7383 - loss: 0.4583 - val_accuracy: 0.2000 - val_loss: 0.9265\n",
      "Epoch 70/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7632 - loss: 0.4582 - val_accuracy: 0.3000 - val_loss: 0.8744\n",
      "Epoch 71/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7788 - loss: 0.4302 - val_accuracy: 0.3000 - val_loss: 0.8317\n",
      "Epoch 72/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8205 - loss: 0.4356 - val_accuracy: 0.4500 - val_loss: 0.7642\n",
      "Epoch 73/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8102 - loss: 0.4378 - val_accuracy: 0.4500 - val_loss: 0.7413\n",
      "Epoch 74/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7619 - loss: 0.4509 - val_accuracy: 0.3500 - val_loss: 0.8102\n",
      "Epoch 75/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8127 - loss: 0.4085 - val_accuracy: 0.2000 - val_loss: 0.9649\n",
      "Epoch 76/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7722 - loss: 0.4282 - val_accuracy: 0.2000 - val_loss: 0.9336\n",
      "Epoch 77/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7566 - loss: 0.4341 - val_accuracy: 0.4000 - val_loss: 0.7754\n",
      "Epoch 78/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8246 - loss: 0.4399 - val_accuracy: 0.6000 - val_loss: 0.6602\n",
      "Epoch 79/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.7854 - loss: 0.4393 - val_accuracy: 0.5500 - val_loss: 0.6648\n",
      "Epoch 80/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.7985 - loss: 0.4286 - val_accuracy: 0.3500 - val_loss: 0.8069\n",
      "Epoch 81/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8154 - loss: 0.4034 - val_accuracy: 0.2500 - val_loss: 0.9043\n",
      "Epoch 82/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8232 - loss: 0.4005 - val_accuracy: 0.3500 - val_loss: 0.8524\n",
      "Epoch 83/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7998 - loss: 0.4095 - val_accuracy: 0.5500 - val_loss: 0.6725\n",
      "Epoch 84/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7985 - loss: 0.4254 - val_accuracy: 0.6500 - val_loss: 0.6110\n",
      "Epoch 85/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7841 - loss: 0.4171 - val_accuracy: 0.4000 - val_loss: 0.7114\n",
      "Epoch 86/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.8651 - loss: 0.3913 - val_accuracy: 0.3500 - val_loss: 0.8384\n",
      "Epoch 87/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.8415 - loss: 0.3891 - val_accuracy: 0.3500 - val_loss: 0.8313\n",
      "Epoch 88/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7893 - loss: 0.3972 - val_accuracy: 0.4000 - val_loss: 0.7072\n",
      "Epoch 89/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.8637 - loss: 0.3764 - val_accuracy: 0.5500 - val_loss: 0.6339\n",
      "Epoch 90/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8703 - loss: 0.3609 - val_accuracy: 0.5000 - val_loss: 0.6647\n",
      "Epoch 91/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8507 - loss: 0.3768 - val_accuracy: 0.4000 - val_loss: 0.7359\n",
      "Epoch 92/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8390 - loss: 0.3894 - val_accuracy: 0.4000 - val_loss: 0.7438\n",
      "Epoch 93/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8429 - loss: 0.3742 - val_accuracy: 0.4000 - val_loss: 0.7029\n",
      "Epoch 94/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8234 - loss: 0.3815 - val_accuracy: 0.6500 - val_loss: 0.6345\n",
      "Epoch 95/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8168 - loss: 0.3998 - val_accuracy: 0.7500 - val_loss: 0.5920\n",
      "Epoch 96/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.8429 - loss: 0.3658 - val_accuracy: 0.7500 - val_loss: 0.6046\n",
      "Epoch 97/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8586 - loss: 0.3557 - val_accuracy: 0.4500 - val_loss: 0.6644\n",
      "Epoch 98/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8351 - loss: 0.3606 - val_accuracy: 0.5000 - val_loss: 0.6620\n",
      "Epoch 99/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8351 - loss: 0.3648 - val_accuracy: 0.7000 - val_loss: 0.5986\n",
      "Epoch 100/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8717 - loss: 0.3490 - val_accuracy: 0.7500 - val_loss: 0.5585\n",
      "Epoch 101/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8639 - loss: 0.3492 - val_accuracy: 0.7500 - val_loss: 0.5588\n",
      "Epoch 102/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8705 - loss: 0.3556 - val_accuracy: 0.5500 - val_loss: 0.6287\n",
      "Epoch 103/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8678 - loss: 0.3426 - val_accuracy: 0.5000 - val_loss: 0.6947\n",
      "Epoch 104/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8522 - loss: 0.3563 - val_accuracy: 0.5000 - val_loss: 0.6720\n",
      "Epoch 105/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.8717 - loss: 0.3308 - val_accuracy: 0.5500 - val_loss: 0.6236\n",
      "Epoch 106/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8717 - loss: 0.3404 - val_accuracy: 0.8000 - val_loss: 0.5388\n",
      "Epoch 107/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9083 - loss: 0.3248 - val_accuracy: 0.8000 - val_loss: 0.4830\n",
      "Epoch 108/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8509 - loss: 0.3538 - val_accuracy: 0.8000 - val_loss: 0.4869\n",
      "Epoch 109/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9161 - loss: 0.3186 - val_accuracy: 0.6500 - val_loss: 0.5909\n",
      "Epoch 110/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.8849 - loss: 0.3253 - val_accuracy: 0.6500 - val_loss: 0.5628\n",
      "Epoch 111/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.8966 - loss: 0.3306 - val_accuracy: 0.8000 - val_loss: 0.5088\n",
      "Epoch 112/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8861 - loss: 0.3119 - val_accuracy: 0.8000 - val_loss: 0.5218\n",
      "Epoch 113/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9030 - loss: 0.2949 - val_accuracy: 0.6000 - val_loss: 0.6027\n",
      "Epoch 114/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8771 - loss: 0.3367 - val_accuracy: 0.8500 - val_loss: 0.4804\n",
      "Epoch 115/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9071 - loss: 0.3147 - val_accuracy: 0.8500 - val_loss: 0.4360\n",
      "Epoch 116/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9110 - loss: 0.3196 - val_accuracy: 0.8000 - val_loss: 0.5172\n",
      "Epoch 117/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9188 - loss: 0.2925 - val_accuracy: 0.7000 - val_loss: 0.5410\n",
      "Epoch 118/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9188 - loss: 0.2883 - val_accuracy: 0.8500 - val_loss: 0.4802\n",
      "Epoch 119/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9149 - loss: 0.2914 - val_accuracy: 0.8500 - val_loss: 0.4482\n",
      "Epoch 120/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9176 - loss: 0.2862 - val_accuracy: 0.8500 - val_loss: 0.4692\n",
      "Epoch 121/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9344 - loss: 0.2760 - val_accuracy: 0.8500 - val_loss: 0.4624\n",
      "Epoch 122/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.8966 - loss: 0.2912 - val_accuracy: 0.9500 - val_loss: 0.3750\n",
      "Epoch 123/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9058 - loss: 0.2985 - val_accuracy: 0.8500 - val_loss: 0.4530\n",
      "Epoch 124/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9110 - loss: 0.2713 - val_accuracy: 0.6500 - val_loss: 0.5708\n",
      "Epoch 125/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8783 - loss: 0.2912 - val_accuracy: 0.8500 - val_loss: 0.4911\n",
      "Epoch 126/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9332 - loss: 0.2601 - val_accuracy: 0.9500 - val_loss: 0.3595\n",
      "Epoch 127/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9122 - loss: 0.2741 - val_accuracy: 0.9500 - val_loss: 0.3323\n",
      "Epoch 128/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9463 - loss: 0.2801 - val_accuracy: 0.7500 - val_loss: 0.4822\n",
      "Epoch 129/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9278 - loss: 0.2511 - val_accuracy: 0.7000 - val_loss: 0.5590\n",
      "Epoch 130/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8993 - loss: 0.2710 - val_accuracy: 0.9500 - val_loss: 0.3474\n",
      "Epoch 131/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9266 - loss: 0.2597 - val_accuracy: 1.0000 - val_loss: 0.2878\n",
      "Epoch 132/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9241 - loss: 0.2754 - val_accuracy: 0.8500 - val_loss: 0.4375\n",
      "Epoch 133/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9305 - loss: 0.2500 - val_accuracy: 0.6500 - val_loss: 0.5706\n",
      "Epoch 134/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9017 - loss: 0.2518 - val_accuracy: 0.8500 - val_loss: 0.4623\n",
      "Epoch 135/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9254 - loss: 0.2564 - val_accuracy: 0.9500 - val_loss: 0.3111\n",
      "Epoch 136/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9476 - loss: 0.2368 - val_accuracy: 1.0000 - val_loss: 0.2745\n",
      "Epoch 137/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9346 - loss: 0.2784 - val_accuracy: 0.9500 - val_loss: 0.3397\n",
      "Epoch 138/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9254 - loss: 0.2425 - val_accuracy: 0.8000 - val_loss: 0.4350\n",
      "Epoch 139/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9137 - loss: 0.2631 - val_accuracy: 0.8000 - val_loss: 0.4506\n",
      "Epoch 140/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9058 - loss: 0.2696 - val_accuracy: 0.8500 - val_loss: 0.4228\n",
      "Epoch 141/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9137 - loss: 0.2512 - val_accuracy: 0.9500 - val_loss: 0.3558\n",
      "Epoch 142/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9137 - loss: 0.2543 - val_accuracy: 1.0000 - val_loss: 0.3091\n",
      "Epoch 143/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9319 - loss: 0.2483 - val_accuracy: 0.9500 - val_loss: 0.3580\n",
      "Epoch 144/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9058 - loss: 0.2562 - val_accuracy: 0.9000 - val_loss: 0.4024\n",
      "Epoch 145/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9215 - loss: 0.2349 - val_accuracy: 0.8000 - val_loss: 0.4195\n",
      "Epoch 146/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9058 - loss: 0.2483 - val_accuracy: 1.0000 - val_loss: 0.2883\n",
      "Epoch 147/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9359 - loss: 0.2283 - val_accuracy: 1.0000 - val_loss: 0.2660\n",
      "Epoch 148/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9371 - loss: 0.2146 - val_accuracy: 1.0000 - val_loss: 0.3070\n",
      "Epoch 149/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9254 - loss: 0.2219 - val_accuracy: 0.9500 - val_loss: 0.3394\n",
      "Epoch 150/150\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9097 - loss: 0.2377 - val_accuracy: 1.0000 - val_loss: 0.2924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Define a simple Sequential model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(34,)),  # Adjust input_dim to match your data\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Use 'softmax' for multi-class classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',  # Use 'categorical_crossentropy' for multi-class classification\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_df, y_df, epochs=150, validation_split=0.2)  # Adjust epochs as needed\n",
    "\n",
    "# Save the model in a format suitable for TensorFlow Lite\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def landmarks_from_squating_images(input_dir,X,Y):\n",
    "    for file in os.listdir(input_dir):\n",
    "        img_path = os.path.join(input_dir, file)\n",
    "        \n",
    "        # Check if the file is an image\n",
    "        if os.path.isfile(img_path) and file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "            img = Image.open(img_path)\n",
    "            img = tf.image.resize_with_pad(np.expand_dims(img,axis=0), 192, 192)\n",
    "            input_image = tf.cast(img, dtype=tf.float32)\n",
    "\n",
    "            # Setup input and output\n",
    "            input_details = interpreter.get_input_details()\n",
    "            output_details = interpreter.get_output_details()\n",
    "\n",
    "            #make predictions\n",
    "            interpreter.set_tensor(input_details[0]['index'], np.array(input_image))\n",
    "            interpreter.invoke()\n",
    "            keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
    "            landmarks = keypoints_with_scores\n",
    "\n",
    "            X.append(landmarks)\n",
    "            Y.append(1)\n",
    "    return X,Y\n",
    "\n",
    "def image_prediction(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    img = tf.image.resize_with_pad(np.expand_dims(img,axis=0), 192, 192)\n",
    "    input_image = tf.cast(img, dtype=tf.float32)\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    #make predictions\n",
    "    interpreter.set_tensor(input_details[0]['index'], np.array(input_image))\n",
    "    interpreter.invoke()\n",
    "    keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
    "    landmarks = keypoints_with_scores\n",
    "    return landmarks[0][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X,test_Y = landmarks_from_squating_images('test images/sit',[],[])\n",
    "test_X,test_Y = landmarks_from_standing_images('test images/stand',test_X,test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        1_x       2_x       3_x       4_x       5_x       6_x       7_x  \\\n",
      "0  0.158883  0.129680  0.128846  0.141852  0.142761  0.247204  0.251513   \n",
      "1  0.422304  0.404254  0.404367  0.414068  0.414410  0.490574  0.495341   \n",
      "2  0.396914  0.384297  0.385278  0.400497  0.403513  0.484471  0.460770   \n",
      "3  0.547260  0.536314  0.538534  0.546335  0.548772  0.611116  0.613522   \n",
      "4  0.148019  0.126063  0.125789  0.158340  0.144929  0.266824  0.281123   \n",
      "5  0.092995  0.069636  0.069766  0.071858  0.072698  0.131020  0.142498   \n",
      "6  0.179094  0.169309  0.169256  0.192265  0.193769  0.293402  0.265297   \n",
      "7  0.355024  0.343350  0.343809  0.354249  0.354406  0.420247  0.419390   \n",
      "8  0.122018  0.101319  0.100482  0.117442  0.109174  0.195384  0.211573   \n",
      "9  0.306403  0.289890  0.288576  0.280730  0.278254  0.337062  0.335010   \n",
      "\n",
      "        8_x       9_x      10_x  ...       8_y       9_y      10_y      11_y  \\\n",
      "0  0.534532  0.501605  0.401277  ...  0.269241  0.339590  0.231762  0.254074   \n",
      "1  0.625781  0.643805  0.515322  ...  0.604323  0.624982  0.652770  0.663743   \n",
      "2  0.598431  0.578372  0.522327  ...  0.660944  0.469270  0.634794  0.581783   \n",
      "3  0.688228  0.697918  0.607606  ...  0.413410  0.662110  0.453288  0.695115   \n",
      "4  0.380627  0.409647  0.267337  ...  0.455894  0.206747  0.345628  0.276372   \n",
      "5  0.287329  0.293203  0.420206  ...  0.507669  0.508359  0.499327  0.501252   \n",
      "6  0.389714  0.372048  0.429880  ...  0.628719  0.394480  0.697744  0.444793   \n",
      "7  0.492874  0.490980  0.444824  ...  0.388984  0.642489  0.402189  0.645673   \n",
      "8  0.267429  0.328119  0.251469  ...  0.556874  0.325422  0.445378  0.388558   \n",
      "9  0.461710  0.465893  0.568288  ...  0.483583  0.489451  0.501375  0.497163   \n",
      "\n",
      "       12_y      13_y      14_y      15_y      16_y      17_y  \n",
      "0  0.690922  0.672519  0.294365  0.307713  0.534263  0.558230  \n",
      "1  0.345256  0.348218  0.538866  0.572163  0.442801  0.463322  \n",
      "2  0.463619  0.360924  0.566067  0.457149  0.498338  0.410601  \n",
      "3  0.475448  0.577770  0.428070  0.652307  0.429497  0.616923  \n",
      "4  0.691065  0.525777  0.528453  0.304899  0.644951  0.399818  \n",
      "5  0.500854  0.503749  0.480244  0.525200  0.513232  0.553748  \n",
      "6  0.511080  0.419761  0.527694  0.446920  0.484829  0.407724  \n",
      "7  0.474317  0.549404  0.443026  0.589325  0.430568  0.612737  \n",
      "8  0.558499  0.477439  0.569109  0.450073  0.616672  0.437894  \n",
      "9  0.472780  0.479109  0.477327  0.481199  0.462648  0.462623  \n",
      "\n",
      "[10 rows x 34 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2001l\\AppData\\Local\\Temp\\ipykernel_46272\\1551579132.py:14: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_test = pd.concat([df_test, pd.DataFrame([new_row])], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "test_columns = [cols for cols in columns if cols != 'Y']\n",
    "\n",
    "df_test = pd.DataFrame(columns=test_columns)\n",
    "\n",
    "for entry in test_X:\n",
    "    new_row = {col: None for col in test_columns}\n",
    "    x = 0\n",
    "    for x in range(1,18):\n",
    "        xName = str(x) + '_x'\n",
    "        yName = str(x) + '_y'\n",
    "        new_row[xName] = entry[0][0][x-1][0]\n",
    "        new_row[yName] = entry[0][0][x-1][1]\n",
    "        x = x+1\n",
    "    df_test = pd.concat([df_test, pd.DataFrame([new_row])], ignore_index=True)\n",
    "print(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.95690054],\n",
       "       [0.95126045],\n",
       "       [0.8693841 ],\n",
       "       [0.98896813],\n",
       "       [0.77883875],\n",
       "       [0.02062225],\n",
       "       [0.07717959],\n",
       "       [0.4932892 ],\n",
       "       [0.07253558],\n",
       "       [0.10814406]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflowjs in c:\\users\\2001l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.18.0)\n",
      "Requirement already satisfied: tensorflow<3,>=2.1.0 in c:\\users\\2001l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflowjs) (2.16.1)\n",
      "Requirement already satisfied: six<2,>=1.12.0 in c:\\users\\2001l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflowjs) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-hub<0.13,>=0.7.0 in c:\\users\\2001l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflowjs) (0.12.0)\n",
      "Requirement already satisfied: packaging~=20.9 in c:\\users\\2001l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflowjs) (20.9)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\2001l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from packaging~=20.9->tensorflowjs) (3.1.2)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\users\\2001l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\2001l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<3,>=2.1.0->tensorflowjs) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\2001l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<3,>=2.1.0->tensorflowjs) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\2001l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<3,>=2.1.0->tensorflowjs) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\2001l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<3,>=2.1.0->tensorflowjs) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\2001l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<3,>=2.1.0->tensorflowjs) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\2001l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<3,>=2.1.0->tensorflowjs) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\2001l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<3,>=2.1.0->tensorflowjs) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\2001l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<3,>=2.1.0->tensorflowjs) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\2001l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<3,>=2.1.0->tensorflowjs) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\2001l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<3,>=2.1.0->tensorflowjs) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\2001l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<3,>=2.1.0->tensorflowjs) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\2001l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<3,>=2.1.0->tensorflowjs) (70.1.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\2001l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<3,>=2.1.0->tensorflowjs) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\2001l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<3,>=2.1.0->tensorflowjs) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\2001l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<3,>=2.1.0->tensorflowjs) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\2001l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<3,>=2.1.0->tensorflowjs) (1.64.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\2001l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<3,>=2.1.0->tensorflowjs) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in c:\\users\\2001l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<3,>=2.1.0->tensorflowjs) (3.3.3)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\users\\2001l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<3,>=2.1.0->tensorflowjs) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\2001l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow<3,>=2.1.0->tensorflowjs) (0.43.0)\n",
      "Requirement already satisfied: rich in c:\\users\\2001l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow<3,>=2.1.0->tensorflowjs) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\2001l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow<3,>=2.1.0->tensorflowjs) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\2001l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow<3,>=2.1.0->tensorflowjs) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\2001l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow<3,>=2.1.0->tensorflowjs) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\2001l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow<3,>=2.1.0->tensorflowjs) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\2001l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow<3,>=2.1.0->tensorflowjs) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\2001l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow<3,>=2.1.0->tensorflowjs) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\2001l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow<3,>=2.1.0->tensorflowjs) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\2001l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow<3,>=2.1.0->tensorflowjs) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\2001l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow<3,>=2.1.0->tensorflowjs) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\2001l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow<3,>=2.1.0->tensorflowjs) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\2001l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow<3,>=2.1.0->tensorflowjs) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\2001l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow<3,>=2.1.0->tensorflowjs) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\2001l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow<3,>=2.1.0->tensorflowjs) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflowjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.compat.v1' has no attribute 'estimator'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m np\u001b[38;5;241m.\u001b[39mfloat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m\n\u001b[0;32m      4\u001b[0m np\u001b[38;5;241m.\u001b[39mbool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflowjs\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfjs\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\2001l\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflowjs\\__init__.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m print_function\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-imports\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflowjs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m converters\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflowjs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m quantization\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflowjs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version\n",
      "File \u001b[1;32mc:\\Users\\2001l\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflowjs\\converters\\__init__.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m print_function\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-imports,line-too-long\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflowjs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconverters\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconverter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflowjs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconverters\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras_h5_conversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save_keras_model\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflowjs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconverters\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras_tfjs_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deserialize_keras_model\n",
      "File \u001b[1;32mc:\\Users\\2001l\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflowjs\\converters\\converter.py:37\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflowjs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconverters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_h5_conversion \u001b[38;5;28;01mas\u001b[39;00m conversion\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflowjs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconverters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_tfjs_loader\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflowjs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconverters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_saved_model_conversion_v2\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdispatch_keras_h5_to_tfjs_layers_model_conversion\u001b[39m(\n\u001b[0;32m     41\u001b[0m     h5_path, output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, quantization_dtype_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     42\u001b[0m     split_weights_by_layer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     43\u001b[0m     weight_shard_size_bytes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m,\n\u001b[0;32m     44\u001b[0m     metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     45\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Converts a Keras HDF5 saved-model file to TensorFlow.js format.\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \n\u001b[0;32m     47\u001b[0m \u001b[38;5;124;03m  Auto-detects saved_model versus weights-only and generates the correct\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03m      groups: an array of weight_groups as defined in tfjs weights_writer.\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\2001l\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflowjs\\converters\\tf_saved_model_conversion_v2.py:42\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model_cli\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_signature_def_map\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjson_format\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MessageToDict\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_hub\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mhub\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflowjs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m write_weights\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflowjs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconverters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m common\n",
      "File \u001b[1;32mc:\\Users\\2001l\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow_hub\\__init__.py:88\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     77\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mThis version of tensorflow_hub requires tensorflow \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     78\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion >= \u001b[39m\u001b[38;5;132;01m{required}\u001b[39;00m\u001b[38;5;124m; Detected an installation of version \u001b[39m\u001b[38;5;132;01m{present}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     82\u001b[0m             required\u001b[38;5;241m=\u001b[39mrequired_tensorflow_version,\n\u001b[0;32m     83\u001b[0m             present\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39m__version__))\n\u001b[0;32m     85\u001b[0m _ensure_tf_install()\n\u001b[1;32m---> 88\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mestimator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LatestModuleExporter\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mestimator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m register_module_for_export\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_column\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m image_embedding_column\n",
      "File \u001b[1;32mc:\\Users\\2001l\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow_hub\\estimator.py:62\u001b[0m\n\u001b[0;32m     55\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     56\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere is already a module registered to be exported as \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     57\u001b[0m           \u001b[38;5;241m%\u001b[39m export_name)\n\u001b[0;32m     58\u001b[0m   tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39madd_to_collection(_EXPORT_MODULES_COLLECTION,\n\u001b[0;32m     59\u001b[0m                                  (export_name, module))\n\u001b[1;32m---> 62\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLatestModuleExporter\u001b[39;00m(\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241m.\u001b[39mExporter):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Regularly exports registered modules into timestamped directories.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m  Warning: Deprecated. This belongs to the hub.Module API and TF1 Hub format.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;124;03m  THIS FUNCTION IS DEPRECATED.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m     91\u001b[0m   \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, serving_input_fn, exports_to_keep\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\2001l\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\util\\module_wrapper.py:232\u001b[0m, in \u001b[0;36mTFModuleWrapper._getattr\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Imports and caches pre-defined API.\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \n\u001b[0;32m    222\u001b[0m \u001b[38;5;124;03mWarns if necessary.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;124;03mfails.\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 232\u001b[0m   attr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tfmw_wrapped_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# Placeholder for Google-internal contrib error\u001b[39;00m\n\u001b[0;32m    236\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfmw_public_apis:\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow.compat.v1' has no attribute 'estimator'"
     ]
    }
   ],
   "source": [
    "np.object = object\n",
    "np.int = int\n",
    "np.float = float\n",
    "np.bool = bool\n",
    "import tensorflowjs as tfjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2114493344.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[27], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    conda create -n tfjs-env numpy==1.19.5\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
